<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anthony Pan | Resampling Methods</title>
  <meta name="description" content="Resampling methods provide ways tools for model assessment (evaulate a model's performance) and model selection (optimally adjust model flexibility).">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Resampling Methods">
  <meta property="og:type" content="website">
  <meta property="og:url" content="anthonypan.com/posts/resampling-methods">
  <meta property="og:description" content="Resampling methods provide ways tools for model assessment (evaulate a model's performance) and model selection (optimally adjust model flexibility).">
  <meta property="og:site_name" content="Anthony Pan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="anthonypan.com/posts/resampling-methods">
  <meta name="twitter:title" content="Resampling Methods">
  <meta name="twitter:description" content="Resampling methods provide ways tools for model assessment (evaulate a model's performance) and model selection (optimally adjust model flexibility).">

  
    <meta property="og:image" content="anthonypan.com/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
    <meta name="twitter:image" content="anthonypan.com/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
  

  <link href="anthonypan.com/feed.xml" type="application/rss+xml" rel="alternate" title="Anthony Pan Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-3ebe45100a3d97f8ac94857224f3fde7193d453226ebbb900022771bfa032719.css">
    

  

</head>
<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav ">
  <a href="/" class="header-logo" title="Anthony Pan">Anthony Pan</a>
  <ul class="header-links">
    
      <li>
        <a href="/" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="/posts/index" title="Blog">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-writing">
  <use href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing" xlink:href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article ">
          <header class="article-header">
            <h1>Resampling Methods</h1>
            <p>Resampling methods provide ways tools for model assessment (evaulate a model's performance) and model selection (optimally adjust model flexibility).</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    April 25, 2020
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      7 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
      
      <a href="/tag/academic" title="See all posts with tag 'academic'">academic</a>
    
  </div>
</div>
          </header>

          <div class="article-content">
            <div class="toc">
  <h3 id="contents">Contents</h3>

  <ul>
    <li><a href="#i-cross-validation">I. Cross-Validation</a></li>
    <li><a href="#ii-bootstrap">II. Bootstrap</a></li>
    <li><a href="#iii-applications-in-r">III. Applications in R</a></li>
  </ul>

</div>

<h3 id="i-cross-validation">I. Cross Validation</h3>

<p>Cross validation involves estimating the test error rate by holding out a subset of the training data. There are three main types of cross validation: using a validation set, leave-one-out cross validation, and k-fold cross validation.</p>

<p><span class="boxheader">Validation Set</span></p>

<p>In this simple method, we split the observations into a training and validation set. We build the model based on the training set, and tune its parameters based on their performance in the validation set.</p>

<p>There are two major drawbacks in this method:</p>
<ol>
  <li>Validation estimates of the test error rate are highly variable depending on the choice of validation set</li>
  <li>only a subset of training data is used to build the model which makes it weaker than it could be, so the validation set often overestimates the test error</li>
</ol>

<p><span class="boxheader">Leave-one-out cross validation</span></p>

<p>Leave-one-out cross validation (LOOCV) uses a single observation for the validation set, fits a model to the rest of the data, then predicts the single validation observation. It then repeats this process for every observation to calculate <script type="math/tex">\text{MSE}_1, ... , \text{MSE}_n</script>. The average of all of these errors is the LOOCV error:</p>

<script type="math/tex; mode=display">CV_{(n)} = \frac{1}{n} \sum_{i = 1}^n \text{MSE}_i</script>

<p>LOOCV can be used for all kinds of predictive modeling, has very little bias and no randomness involved, but it can be resource intensive.</p>

<p>For least squares linear or polynomial regression, there is an easy shortcut to solve for the LOOCV error for computational efficiency (same cost as a single model fit):</p>

<script type="math/tex; mode=display">CV_{(n)} = \frac{1}{n} \sum_{i = 1}^n \biggl(\frac {y_i - \hat y_i} {1 - h_i} \biggr)^2 , \\
\text{ where } h_i \text{ is the leverage statistic for observation } i.</script>

<p><span class="boxheader">K-fold cross validation</span></p>

<p>K-Fold cross validation randomly divides the observation into K folds of approximately equal size, has each one serve as the validation set for the rest. The test errors are then averaged to calculate the K-fold cross validation error.</p>

<script type="math/tex; mode=display">CV_{(k)} = \frac{1}{K} \sum_{i = 1}^K \text{MSE}_i</script>

<p>This is more computationally feasible than LOOCV, and has higher bias and lower variance. This is because the model outputs of LOOCV are highly correlated with each other, while those of K-fold CV are somewhat less correlated with each other, and the mean of highly correlated quantities has higher variance. K-fold cross validation often actually outperforms LOOCV due to the bias/variance tradeoff.</p>

<p>Note: Use K = 5 or K = 10 in practice for the best results.</p>

<p><span class="boxheader">Classification vs. Regression</span></p>

<p>The previous formulas assume that we’re dealing with regression and use mean squared error as the measurement of performance. For classifcation, we can use <script type="math/tex">Err_i = I(y_i \neq \hat y_i)</script> to represent mischaracterized observations instead of MSE.</p>

<p>For example, LOOCV for logistic regression looks like this:</p>

<script type="math/tex; mode=display">CV_{(n)} = \frac{1}{n} \sum_{i = 1}^n Err_i</script>

<hr />
<hr />
<hr />
<h3 id="ii-bootstrap">II. Bootstrap</h3>

<p>Bootstrap allows us to quantify the uncertainty associated with an estimator or model. It is particularly useful when we can’t calculate the standard error of an estimator (i.e. median), or when we can’t assume anything about the population distribution (i.e. normality).</p>

<ul>
  <li>Sample with replacement a large number of times, <script type="math/tex">B</script> on a data set <script type="math/tex">Z</script> to obtain <script type="math/tex">Z^{*1}, Z^{*2}, ... , Z^{*B}</script>.</li>
  <li>For each sample, calculate the bootstrap estimates <script type="math/tex">\hat \alpha^{*1}, \hat \alpha^{*2}, ... , \hat \alpha^{*B},</script></li>
  <li>Then calculate the standard error:</li>
</ul>

<script type="math/tex; mode=display">SE_B (\hat \alpha) = \sqrt {\frac{1}{B - 1} \sum_{r = 1}^B \biggl( \hat \alpha ^ {*r} - \frac{1}{B} \sum_{r' = 1}^B \hat \alpha ^ {*r'} \biggr) ^ 2}</script>

<p>When choosing between models, the “one standard error rule” is used, and we choose the most sparse model who error is no more than one standard error above that of the best performing model.</p>

<p>Note that the probability that a bootstrap sample of size <script type="math/tex">n</script> contains the <script type="math/tex">j</script>th observation is <script type="math/tex">p = 1 - (1 - \frac{1}{n})^n</script>. Since <script type="math/tex">\lim \limits_{n \to \infty} (1 + \frac{x}{n})^n = e^x</script>,  <script type="math/tex">p</script> converges to <script type="math/tex">1 - \frac{1}{e} = .632</script> as <script type="math/tex">n \to \infty</script></p>

<hr />
<hr />
<hr />
<h3 id="iii-applications-in-r">III. Applications in R</h3>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="s2">"ISLR"</span><span class="p">)</span><span class="w"> </span><span class="c1"># sample data set</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"boot"</span><span class="p">)</span><span class="w"> </span><span class="c1"># bootstrap function</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">### Validation set</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="m">392</span><span class="p">,</span><span class="w"> </span><span class="m">196</span><span class="p">)</span><span class="w">

</span><span class="c1">### Leave one out cross validation</span><span class="w">
</span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">horsepower</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Auto</span><span class="p">)</span><span class="w">
</span><span class="n">loocv_error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glm</span><span class="p">(</span><span class="n">Auto</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">)</span><span class="w">
</span><span class="n">loocv_error</span><span class="o">$</span><span class="n">delta</span><span class="w"> </span><span class="c1"># loocv error</span><span class="w">

</span><span class="c1"># loocv for five competing models</span><span class="w">
</span><span class="n">loocv_error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">poly</span><span class="p">(</span><span class="n">horsepower</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Auto</span><span class="p">)</span><span class="w">
  </span><span class="n">loocv_error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glm</span><span class="p">(</span><span class="n">Auto</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">)</span><span class="o">$</span><span class="n">delta</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">loocv_error</span><span class="w"> </span><span class="c1"># 2nd order polynomial is best</span><span class="w">

</span><span class="c1">### K-fold cross-valudation</span><span class="w">
</span><span class="n">kfoldcv_error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">poly</span><span class="p">(</span><span class="n">horsepower</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Auto</span><span class="p">)</span><span class="w">
  </span><span class="n">kfoldcv_error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glm</span><span class="p">(</span><span class="n">Auto</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="o">$</span><span class="n">delta</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">kfoldcv_error</span><span class="w">

</span><span class="c1">## Bootstrap</span><span class="w">
</span><span class="n">bootstrap_stat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">coef</span><span class="p">(</span><span class="n">lm</span><span class="p">(</span><span class="n">mpg</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">horsepower</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">index</span><span class="p">)))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">boot</span><span class="p">(</span><span class="n">Auto</span><span class="p">,</span><span class="w"> </span><span class="n">bootstrap_stat</span><span class="p">,</span><span class="w"> </span><span class="m">1000</span><span class="p">)</span><span class="w">

</span><span class="c1">### Additional exercise: Validation set / bootstrap with "default" data</span><span class="w">

</span><span class="n">default</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Default</span><span class="w">

</span><span class="c1"># validation set</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">default</span><span class="p">)[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">default</span><span class="p">)[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># list of indexes for training set</span><span class="w">
</span><span class="n">model_logit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">default</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">income</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">balance</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">default</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binomial"</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit logistic regression</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">model_logit</span><span class="p">)</span><span class="w"> 
  
</span><span class="n">probs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_logit</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">default</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="p">],</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"response"</span><span class="p">)</span><span class="w"> </span><span class="c1"># probs that test set is "Yes" based on training model</span><span class="w">
</span><span class="n">pred_logit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="s2">"No"</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span><span class="w"> 
</span><span class="n">pred_logit</span><span class="p">[</span><span class="n">probs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0.5</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"Yes"</span><span class="w"> </span><span class="c1"># classify to default category if posterior probability is &gt; 0.5</span><span class="w">

</span><span class="n">mean</span><span class="p">(</span><span class="n">pred_logit</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">default</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="o">$</span><span class="n">default</span><span class="p">)</span><span class="w"> </span><span class="c1"># calculate test error rate for prediction results to actual results</span><span class="w">

</span><span class="c1"># bootstrap</span><span class="w">
</span><span class="n">bootstrap_stat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">coef</span><span class="p">(</span><span class="n">glm</span><span class="p">(</span><span class="n">default</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">income</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">balance</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binomial"</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">index</span><span class="p">)))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">bootstrap_est</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">boot</span><span class="p">(</span><span class="n">Default</span><span class="p">,</span><span class="w"> </span><span class="n">bootstrap_stat</span><span class="p">,</span><span class="w"> </span><span class="m">1000</span><span class="p">)</span><span class="w">  </span></code></pre></figure>


          </div>
          <!--
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Resampling+Methods%20-%20anthonypan.com/posts/resampling-methods" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=anthonypan.com/posts/resampling-methods" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>
          -->
          <span class="backbutton"><a href="/posts/index">←Index</a></span>
          
        </article>
        <footer class="footer ">

<small class="pull-left"> &copy;2020 All rights reserved. 

<a href="https://github.com/nielsenramon/chalk" target="_blank">Chalk</a> theme by Nielson Ramon.

</small>
</footer>

      </div>
    </div>
  </main>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160038909-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-160038909-1');
  </script>


<script src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>




<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }
   });
   MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
     var TEX = MathJax.InputJax.TeX;
     var COLS = function (W) {
       var WW = [];
       for (var i = 0, m = W.length; i < m; i++)
         {WW[i] = TEX.Parse.prototype.Em(W[i])}
       return WW.join(" ");
     };
     TEX.Definitions.Add({
       environment: {
         psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
       }
     });
   });
 </script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
