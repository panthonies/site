<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anthony Pan | Tree-Based Methods</title>
  <meta name="description" content="Decision trees, which divide the predictor space into regions, are simple and useful for interpretation. Their predictive power can be improved with bagging, random forests, and boosting.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Tree-Based Methods">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/posts/tree-based-methods">
  <meta property="og:description" content="Decision trees, which divide the predictor space into regions, are simple and useful for interpretation. Their predictive power can be improved with bagging, random forests, and boosting.">
  <meta property="og:site_name" content="Anthony Pan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://localhost:4000/posts/tree-based-methods">
  <meta name="twitter:title" content="Tree-Based Methods">
  <meta name="twitter:description" content="Decision trees, which divide the predictor space into regions, are simple and useful for interpretation. Their predictive power can be improved with bagging, random forests, and boosting.">

  
    <meta property="og:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
    <meta name="twitter:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
  

  <link href="http://localhost:4000/feed.xml" type="application/rss+xml" rel="alternate" title="Anthony Pan Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-3ebe45100a3d97f8ac94857224f3fde7193d453226ebbb900022771bfa032719.css">
    

  

</head>
<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav ">
  <a href="/" class="header-logo" title="Anthony Pan">Anthony Pan</a>
  <ul class="header-links">
    
      <li>
        <a href="/" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="/posts/index" title="Blog">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-writing">
  <use href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing" xlink:href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article ">
          <header class="article-header">
            <h1>Tree-Based Methods</h1>
            <p>Decision trees, which divide the predictor space into regions, are simple and useful for interpretation. Their predictive power can be improved with bagging, random forests, and boosting.</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    May 1, 2020
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      13 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
      
      <a href="/tag/academic" title="See all posts with tag 'academic'">academic</a>
    
  </div>
</div>
          </header>

          <div class="article-content">
            <div class="toc">
  <h3 id="contents">Contents</h3>

  <ul>
    <li><a href="#i-overview-of-decision-trees">I. Overview of Decision Trees</a></li>
    <li><a href="#ii-regression-trees">II. Regression Trees</a></li>
    <li><a href="#iii-classification-trees">III. Classification Trees</a></li>
    <li><a href="#iv-method-bagging">IV. Methods: Bagging</a></li>
    <li><a href="#v-method-random-forests">V.  Methods: Random Forests</a></li>
    <li><a href="#vi-method-boosting">VI. Methods: Boosting</a></li>
    <li><a href="#vii-applications-in-r">VII. Applications in R</a></li>
  </ul>

</div>

<h3 id="i-overview-of-decision-trees">I. Overview of Decision Trees</h3>

<p>Trees involve stratifying or segmenting the predictor space into a number of simple regions, where the prediction of an observation is typically the mean or mode of the training observations in the region to which it belongs.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{Trees: } f(x) & = \sum_{m = 1}^M c_m * 1_{(x \in R_m)}\\
\text{Linear models: } f(x) & = \beta_0 + \sum_{j = 1}^p x_j \beta_j
\end{align} %]]></script>

<p><strong>Pros:</strong></p>
<ul>
  <li>Easy to explain</li>
  <li>Can be displayed graphically and are easily interpreted</li>
  <li>Can easily handle qualitative predictors without dummy variables</li>
  <li>May more closely mirror human decision-making than least squares or other classical models</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Not very robust to new observations</li>
  <li>In general, lower predictive accuracy than other regression and classification approaches unless we use bagging, random forests, and boosting, which increase predictive performance but decrease interpretability</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="ii-regression-trees">II. Regression Trees</h3>

<p>A regression tree is typically drawn upside down, with the terminal nodes (leaves) at the bottom. The predictor space is split at a number of internal nodes connected by branches. To build a regression tree:</p>

<ol>
  <li>Divide the predictor space, the set of all posible values for <script type="math/tex">X_1, X_2, ..., X_p</script> into <script type="math/tex">J</script> distinct, non-overlapping regions, <script type="math/tex">R_1, R_2, ..., R_J</script>.</li>
  <li>For every observation that falls into region <script type="math/tex">R_j</script>, predict the response as the mean of the response values for the training observations in <script type="math/tex">R_j</script>.</li>
</ol>

<p>We choose the regions to minimize the RSS: <script type="math/tex">\sum_{j = 1}^J \sum_{i \in R_j} (y_i - \hat y_{R_j}) ^ 2</script>, where <script type="math/tex">\hat y_{R_j}</script> is the mean response of training observations in the <script type="math/tex">j</script>th box. This is a top down, greedy approach called <strong>recursive binary splitting.</strong><sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<p><span class="boxheader">Recursive Binary Splitting</span></p>

<p>To perform recursive binary splitting, we consider <script type="math/tex">X_1, ..., X_p</script> and all possible cutpoints <script type="math/tex">s</script> for each predictor, then choose the predictor and cutpoint to minimize RSS. In formal terms,</p>

<script type="math/tex; mode=display">% <![CDATA[
\text{Define half-planes } R_1(j, s) = \{ X \vert X_j < s \} \text{ and } R_2(j, s) = \{ X \vert X_j \geq s \} %]]></script>

<script type="math/tex; mode=display">\min_{j, s} \sum_{i: x \in R_{1(j, s)}} (y_i - \hat y_{R_1}) ^ 2 + \sum_{i: x \in R_{2(j, s)}} (y_i - \hat y_{R_2}) ^ 2</script>

<p>Where <script type="math/tex">% <![CDATA[
\{ X \vert X_j < s \} %]]></script> represents the region of the predictor space in which <script type="math/tex">X_j</script> takes as value less than <script type="math/tex">s</script>, and <script type="math/tex">\hat y_{R_1}</script> and <script type="math/tex">\hat y_{R_2}</script> are the mean responses for the training observations in <script type="math/tex">R_1 (j, s)</script> and <script type="math/tex">R_2 (j, s)</script>, respectively. We repeat this process by splitting one of the two previously identified regions until a stopping condition is reached.</p>

<p><span class="boxheader">Cost Complexity Pruning</span></p>

<p>The recursive binary splitting process is likely to create an overly complex tree and overfit the data. To address this, we can lower the variance with cost-complexity pruning or weakest link pruning with a nonnegative tuning parameter <script type="math/tex">\alpha</script>:</p>

<p>For each value of <script type="math/tex">\alpha</script>, <script type="math/tex">\exists T \subset T_0</script> such that the following equation is minimized:</p>

<script type="math/tex; mode=display">\sum_{m = 1}^t \sum_{i: x \in R_m} (y_i - \hat y_{R_m}) ^ 2 + \alpha \vert T \vert</script>

<ul>
  <li><script type="math/tex">\vert T \vert</script> is the number of terminal notes</li>
  <li><script type="math/tex">R_m</script> is the subset of predictor space corresponding to the <script type="math/tex">m</script>th terminal node</li>
  <li><script type="math/tex">\hat y_{R_m}</script> is the predicted response</li>
  <li><script type="math/tex">\alpha</script> controls the tradeoff beween complexity and fit; as <script type="math/tex">\alpha \to \infty</script>, a small subtree is chosen</li>
</ul>

<p>We choose <script type="math/tex">\alpha</script> with K-fold cross validation, averaging results for each value of <script type="math/tex">\alpha</script>, and selecting <script type="math/tex">\alpha</script> to minimize the average error.</p>

<p><span class="boxheader">Summarized Algorithm:</span></p>
<ol>
  <li>Use recursive binary pruning to grow a large tree until each terminal node reaches <script type="math/tex">X</script> minimum number of observations.</li>
  <li>Apply cost-complexity pruning to obtain a sequence of best subtees as a functio of <script type="math/tex">\alpha</script>.</li>
  <li>Use k-fold cross-valudation to choose <script type="math/tex">\alpha</script> by repeating the first two steps on the <script type="math/tex">k</script>th fold and averaging all of the MSEs.</li>
  <li>Return the subtree from step 2 for the chosen best value of <script type="math/tex">\alpha</script>.</li>
</ol>

<hr />
<hr />
<hr />
<h3 id="iii-classification-trees">III. Classification Trees</h3>

<p>Classification trees are similar to regression trees; they are grown with recursive binary splitting (refer to the previous section). The predicted class is chosen by the most common class in the region.</p>

<p>However, we need an alternative to RSS to use as criterion for making the binary splits. There are a several types of error we can choose to minimize when building classification trees:</p>

<ul>
  <li><strong>Classification error</strong>
    <ul>
      <li><script type="math/tex">E = 1 - \max_k (\hat p_{mk})</script>, where <script type="math/tex">\hat p_{mk}</script> is the proportion of training observations in region <script type="math/tex">m</script> from class <script type="math/tex">k</script>.</li>
      <li>this type of error is not very good to use with trees</li>
    </ul>
  </li>
  <li><strong>Gini Index</strong>
    <ul>
      <li><script type="math/tex">G = \sum_{k = 1}^K \hat p_{mk} (1 = \hat p_{mk})</script> ;</li>
      <li>this is a measure of total variance across the <script type="math/tex">K</script> classes</li>
      <li>measures node purity; a smaller value means that the node contains predominantly observations from a single class</li>
      <li>node purity is important because it is a marker for prediction certainty</li>
    </ul>
  </li>
  <li><strong>Entropy</strong>
    <ul>
      <li><script type="math/tex">D = - \sum_{k = 1}^K \hat p_{mk} \log \hat p_{mk}</script> ;</li>
      <li>note that entropy is always positive, since <script type="math/tex">- \hat p_{mk} \log \hat p_{mk} > 0</script></li>
      <li>this is small if <script type="math/tex">\hat p_{mk}</script>’s are all near 0 or all near 1 (i.e. if the <script type="math/tex">m</script>th node is pure)</li>
      <li>similar to the Gini index, can be used as an alternative</li>
    </ul>
  </li>
</ul>

<hr />
<hr />
<hr />
<h3 id="iv-method-bagging">IV. Method: Bagging</h3>

<p>Bagging, or bootstrap aggregation, reduces the variance of a statistical learning method. Decision trees suffer from high variance, so we can reduce the variance by taking many training sets from the population.</p>

<p>Generate <script type="math/tex">B</script> different bootstrapped data sets, train the method on the <script type="math/tex">b</script>th data set to obtain <script type="math/tex">\hat f ^ {*b} (x)</script>, then average all of the predictions to obtain the function obtained from bagging:</p>

<script type="math/tex; mode=display">\hat f_{\text{bag}} (x) = \frac{1}{B} \sum_{b = 1}^B \hat f ^ {*b} (x)</script>

<p>For qualitative responses, the overall prediction of the bagging tree is is the most commonly occurring class among the <script type="math/tex">B</script> predictions.</p>

<p><script type="math/tex">B</script>, the number of trees, is not a critical parameter! In practice, we just need to use a large B so that the error has settled down, which is typically a value greater than 100.</p>

<p><strong>Out-of-bag Error Estimation</strong> is a straightforward way to estimate the test error of a bagged model.</p>
<ul>
  <li>Each bagged tree makes use of about 2/3 of the data.</li>
  <li>For the <script type="math/tex">i</script>th observation, there are about <script type="math/tex">B</script>/3 predictions for <script type="math/tex">i</script> in which <script type="math/tex">i</script> is <em>out-of-bag</em>.</li>
  <li>Obtain a single prediction for the <script type="math/tex">i</script>th observation with an average (if regression) or majority vote (if classification)</li>
  <li>Calculate the out-of-bag MSE to estimate the test error. This is virtually equivalent to LOOCV.</li>
</ul>

<p><strong>Variable importance measures</strong></p>
<ul>
  <li>Note that bagging increases prediction accuracy, but lowers interpretability.</li>
  <li>To measure variable importance for <strong>regression</strong>, take the average over all <script type="math/tex">B</script> trees of the total amount that RSS degreases due to splits over a given predictor.</li>
  <li>To measure variable importance for <strong>classification</strong>, take the average over all <script type="math/tex">B</script> trees of that total amount that the Gini index decreases by splits over a given predictor.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="v-method-random-forests">V. Method: Random Forests</h3>

<p>Random Forests are an extension of bagging, where we generate <script type="math/tex">B</script> different bootstrapped data sets. However, each time a split is considered, a random sample of <script type="math/tex">m</script> predictors is chosen as split candidates. Usually, <script type="math/tex">m \approx \sqrt p</script>.</p>

<ul>
  <li>If there is 1 very strong predictor, then bagging will not substantially reduce the variance of the tree fit.</li>
  <li>In random forests, <script type="math/tex">\frac{p - m}{p}</script> predictors will not consider the strong predictor, which decorrelates the trees, making the average less variable and more reliable. In particular, use a small value of <script type="math/tex">m</script> when there are a large number of correlated variables.</li>
  <li>Like bagging, any value of <script type="math/tex">B</script> will not overfit the data, so choose <script type="math/tex">B</script> such that the error has settled down.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="vi-method-boosting">VI. Method: Boosting</h3>

<p>Boosting is a general approach that can be used for many statistical learning methods, including decision trees. It works similarly to bagging, except trees are grown sequentially, and fit on a modified version of the original data set.</p>

<p>There are three <strong>tuning parameters</strong> with boosting:</p>
<ol>
  <li>The number of trees, <script type="math/tex">B</script>.
    <ul>
      <li>choose with cross validation, since a large <script type="math/tex">B</script> can lead to overfitting (slowly)</li>
    </ul>
  </li>
  <li>The shrinkage parameter, <script type="math/tex">\lambda</script>, which controls the rate at which boosting learns.
    <ul>
      <li>typically set to .01 or .001</li>
      <li>note that a very small <script type="math/tex">\lambda</script> can require a very large <script type="math/tex">B</script> for good performance</li>
    </ul>
  </li>
  <li>The number of splits in each tree, <script type="math/tex">d</script>, which controls the complexity of the boosted trees
    <ul>
      <li>often, <script type="math/tex">d = 1</script> works well; this is when each tree is a stump with a single split</li>
      <li><script type="math/tex">d</script> is also the interaction depth, and controls the interaction order of boosted models</li>
    </ul>
  </li>
</ol>

<p><strong>Algorithm: Boosting for Regression Trees</strong></p>
<ol>
  <li>Set <script type="math/tex">\hat f(x) = 0</script> , and <script type="math/tex">r_i = y_i</script> for all <script type="math/tex">i</script> in the training set</li>
  <li>For <script type="math/tex">b = 1, 2, ..., B</script>:
    <ul>
      <li>Fit a tree <script type="math/tex">\hat f^b</script> with <script type="math/tex">d</script> splits (<script type="math/tex">d+1</script> terminal nodes) to the training data <script type="math/tex">(X, r)</script></li>
      <li>Update <script type="math/tex">\hat f</script> by adding a shrunken version of the new tree: <script type="math/tex">\hat f(x) \leftarrow \hat f(x) + \lambda \hat f^b(x_i)</script></li>
      <li>Update the residuals: <script type="math/tex">r_i \leftarrow r_i - \lambda \hat f^b(x_i)</script></li>
    </ul>
  </li>
  <li>Output the boosted model:</li>
</ol>

<script type="math/tex; mode=display">\hat f(x) = \sum_{b = 1}^B \lambda \hat f^b(x)</script>

<p>Boosting learns slowly by fitting decision trees to the residuals of the current tree. Trees are grown sequentially, and the shrinkage parameter <script type="math/tex">\lambda</script> slows down the process.</p>

<p>Note: smaller trees can help with interpretability. For example, using stumps can also be interpreted as creating an additive model.</p>

<hr />
<hr />
<hr />
<h3 id="vii-applications-in-r">VII. Applications in R</h3>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="s2">"ISLR"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"tree"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"caret"</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">attach</span><span class="p">(</span><span class="n">Carseats</span><span class="p">)</span><span class="w">

</span><span class="c1">### fitting classification trees</span><span class="w">

</span><span class="c1"># prepare data</span><span class="w">
</span><span class="n">High</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ifelse</span><span class="p">(</span><span class="n">Sales</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="s2">"No"</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yes"</span><span class="p">)</span><span class="w">
</span><span class="n">Carseats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">Carseats</span><span class="p">,</span><span class="w"> </span><span class="n">High</span><span class="p">)</span><span class="w">

</span><span class="c1"># using the tree() function</span><span class="w">
</span><span class="n">tree_carseats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">High</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="o">-</span><span class="n">Sales</span><span class="p">,</span><span class="w"> </span><span class="n">Carseats</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">tree_carseats</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">tree_carseats</span><span class="p">)</span><span class="w"> </span><span class="c1"># display tree structure</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">tree_carseats</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="c1"># display node labels</span><span class="w">

</span><span class="c1"># split training and test</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">Carseats</span><span class="p">),</span><span class="w"> </span><span class="m">200</span><span class="p">)</span><span class="w">
</span><span class="n">test_Carseats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Carseats</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,]</span><span class="w">
</span><span class="n">test_High</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">High</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">]</span><span class="w">

</span><span class="c1"># run model on training data</span><span class="w">
</span><span class="n">tree_carseats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">High</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Sales</span><span class="p">,</span><span class="w"> </span><span class="n">Carseats</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">
</span><span class="n">tree_pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">tree_carseats</span><span class="p">,</span><span class="w"> </span><span class="n">test_Carseats</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"class"</span><span class="p">)</span><span class="w">

</span><span class="n">table</span><span class="p">(</span><span class="n">tree_pred</span><span class="p">,</span><span class="w"> </span><span class="n">test_High</span><span class="p">)</span><span class="w"> </span><span class="c1"># correct prediction rate = .7</span><span class="w">

</span><span class="c1"># cross validation for tree pruning to select final # of nodes</span><span class="w">
</span><span class="n">cv_carseats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.tree</span><span class="p">(</span><span class="n">tree_carseats</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prune.misclass</span><span class="p">)</span><span class="w">
</span><span class="n">cv_carseats</span><span class="w"> </span><span class="c1"># lowest cv error rate (dev) = tree with 9 terminal nodes</span><span class="w">

</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_carseats</span><span class="o">$</span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cv_carseats</span><span class="o">$</span><span class="n">dev</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">)</span><span class="w"> </span><span class="c1"># plot cv error rate vs tree size</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_carseats</span><span class="o">$</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">cv_carseats</span><span class="o">$</span><span class="n">dev</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">)</span><span class="w"> </span><span class="c1"># plot cv error rate vs number of terminal nodes</span><span class="w">

</span><span class="n">prune_carseats</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prune.misclass</span><span class="p">(</span><span class="n">tree_carseats</span><span class="p">,</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">prune_carseats</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">prune_carseats</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="c1"># check prediction on test data set</span><span class="w">
</span><span class="n">tree_pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">prune_carseats</span><span class="p">,</span><span class="w"> </span><span class="n">test_Carseats</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"class"</span><span class="p">)</span><span class="w">
</span><span class="n">table</span><span class="p">(</span><span class="n">tree_pred</span><span class="p">,</span><span class="w"> </span><span class="n">test_High</span><span class="p">)</span><span class="w"> </span><span class="c1"># correct prediction rate = .72</span><span class="w">


</span><span class="c1">### fitting regression trees</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"MASS"</span><span class="p">)</span><span class="w">

</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">Boston</span><span class="p">),</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">Boston</span><span class="p">)</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="w">

</span><span class="n">tree_boston</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">medv</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">Boston</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">tree_boston</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">tree_boston</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">tree_boston</span><span class="p">,</span><span class="w"> </span><span class="n">pretty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">

</span><span class="c1"># cross validation to prune tree</span><span class="w">
</span><span class="n">cv_boston</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.tree</span><span class="p">(</span><span class="n">tree_boston</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_boston</span><span class="o">$</span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cv_boston</span><span class="o">$</span><span class="n">dev</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">)</span><span class="w"> </span><span class="c1"># pruning does not help</span><span class="w">

</span><span class="c1"># if you wanted to prune, you would do this:</span><span class="w">
</span><span class="c1">#prune_boston &lt;- prune.tree(tree_boston, best = 7)</span><span class="w">
</span><span class="c1">#plot(prune_boston)</span><span class="w">
</span><span class="c1">#text(prune_boston, pretty = 0)</span><span class="w">

</span><span class="c1"># predict the test set</span><span class="w">
</span><span class="n">tree_predict</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">tree_boston</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,])</span><span class="w">
</span><span class="n">test_boston</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Boston</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,</span><span class="s2">"medv"</span><span class="p">]</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">tree_predict</span><span class="p">,</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">mean</span><span class="p">((</span><span class="n">tree_predict</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># test MSE = 35.3</span><span class="w">


</span><span class="c1">### bagging and random forests</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"randomForest"</span><span class="p">)</span><span class="w">

</span><span class="c1"># bagging</span><span class="w">
</span><span class="n">bag_boston</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">medv</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">13</span><span class="p">,</span><span class="w"> </span><span class="n">importance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">bag_boston</span><span class="w">

</span><span class="n">bag_predict</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">bag_boston</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,])</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">bag_predict</span><span class="p">,</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">mean</span><span class="p">((</span><span class="n">bag_predict</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># test MSE: 23.5</span><span class="w">

</span><span class="c1"># random forest - change the mtry parameter</span><span class="w">
</span><span class="n">rf_boston</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">medv</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="n">importance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">rf_predict</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">rf_boston</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,])</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">rf_predict</span><span class="p">,</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="w">
</span><span class="n">abline</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">mean</span><span class="p">((</span><span class="n">rf_predict</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># test MSE: 20.2</span><span class="w">

</span><span class="c1"># rank the importance of each variable</span><span class="w">
</span><span class="n">importance</span><span class="p">(</span><span class="n">rf_boston</span><span class="p">)</span><span class="w">
</span><span class="n">varImpPlot</span><span class="p">(</span><span class="n">rf_boston</span><span class="p">)</span><span class="w">

</span><span class="c1">### boosting</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"gbm"</span><span class="p">)</span><span class="w">

</span><span class="c1"># note: set distribution = "gaussian" for regression, "bernoulli" for classification</span><span class="w">

</span><span class="n">boost_boston</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="n">medv</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> 
                    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">[</span><span class="n">train</span><span class="p">,],</span><span class="w"> 
                    </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="c1"># gaussion = regression</span><span class="w">
                    </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">            </span><span class="c1"># 5000 trees, default 100</span><span class="w">
                    </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w">     </span><span class="c1"># limits the depth of each tree to 4, default 1</span><span class="w">
                    </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.2</span><span class="p">)</span><span class="w">          </span><span class="c1"># shrinkage parameter, default .1</span><span class="w">

</span><span class="c1"># variable importance plots</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">boost_boston</span><span class="p">)</span><span class="w"> </span><span class="c1"># relative influence statistics</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w"> 
</span><span class="n">plot</span><span class="p">(</span><span class="n">boost_boston</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"rm"</span><span class="p">)</span><span class="w"> </span><span class="c1"># partial dependence plots</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">boost_boston</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lstat"</span><span class="p">)</span><span class="w">

</span><span class="c1"># predict medv in test set</span><span class="w">
</span><span class="n">boost_predict</span><span class="w"> </span><span class="o">&lt;-</span><span class="w">  </span><span class="n">predict</span><span class="p">(</span><span class="n">boost_boston</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Boston</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,],</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">)</span><span class="w">
</span><span class="n">mean</span><span class="p">((</span><span class="n">boost_predict</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_boston</span><span class="p">)</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="c1"># test MSE: 17.1</span></code></pre></figure>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Recursive binary splitting is top-down because it begins at the top of the tree where all observations begin at a single region. It is greedy because at each step of the tree-building process, the best split is made at that step, rather than looking ahead and picking a split that will lead to a better tree in a future step. <a href="#fnref:1" class="reversefootnote">⤴</a></p>
    </li>
  </ol>
</div>

          </div>
          <!--
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Tree-Based+Methods%20-%20http://localhost:4000/posts/tree-based-methods" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/tree-based-methods" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>
          -->
          <span class="backbutton"><a href="/posts/index">←Index</a></span>
          
        </article>
        <footer class="footer ">

<small class="pull-left"> &copy;2019 All rights reserved. 

<a href="https://github.com/nielsenramon/chalk" target="_blank">Chalk</a> theme by Nielson Ramon.

</small>
</footer>

      </div>
    </div>
  </main>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160038909-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-160038909-1');
  </script>


<script src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>




<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }
   });
   MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
     var TEX = MathJax.InputJax.TeX;
     var COLS = function (W) {
       var WW = [];
       for (var i = 0, m = W.length; i < m; i++)
         {WW[i] = TEX.Parse.prototype.Em(W[i])}
       return WW.join(" ");
     };
     TEX.Definitions.Add({
       environment: {
         psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
       }
     });
   });
 </script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
