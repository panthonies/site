<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anthony Pan | Linear Regression</title>
  <meta name="description" content="Linear Regression is a simple approach for predicting a quantitative response, and is the foundation of many other types of statistical learning. It is especially convenient for establishing a the strength of relationships between specific variables to an output, and is easily interpretable.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Linear Regression">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/posts/linear-regression">
  <meta property="og:description" content="Linear Regression is a simple approach for predicting a quantitative response, and is the foundation of many other types of statistical learning. It is especially convenient for establishing a the strength of relationships between specific variables to an output, and is easily interpretable.">
  <meta property="og:site_name" content="Anthony Pan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://localhost:4000/posts/linear-regression">
  <meta name="twitter:title" content="Linear Regression">
  <meta name="twitter:description" content="Linear Regression is a simple approach for predicting a quantitative response, and is the foundation of many other types of statistical learning. It is especially convenient for establishing a the strength of relationships between specific variables to an output, and is easily interpretable.">

  
    <meta property="og:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
    <meta name="twitter:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
  

  <link href="http://localhost:4000/feed.xml" type="application/rss+xml" rel="alternate" title="Anthony Pan Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-3ebe45100a3d97f8ac94857224f3fde7193d453226ebbb900022771bfa032719.css">
    

  

</head>
<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav ">
  <a href="/" class="header-logo" title="Anthony Pan">Anthony Pan</a>
  <ul class="header-links">
    
      <li>
        <a href="/" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="/posts/index" title="Blog">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-writing">
  <use href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing" xlink:href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article ">
          <header class="article-header">
            <h1>Linear Regression</h1>
            <p>Linear Regression is a simple approach for predicting a quantitative response, and is the foundation of many other types of statistical learning. It is especially convenient for establishing a the strength of relationships between specific variables to an output, and is easily interpretable.</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    April 22, 2020
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      10 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
      
      <a href="/tag/academic" title="See all posts with tag 'academic'">academic</a>
    
  </div>
</div>
          </header>

          <div class="article-content">
            <div class="toc">
  <h3 id="contents">Contents</h3>

  <ul>
    <li><a href="#i-simple-linear-regression">I. Simple Linear Regression</a></li>
    <li><a href="#ii-multiple-linear-regression">II. Multiple Linear Regression</a></li>
    <li><a href="#iii-considerations-and-potential-problems">III. Considerations and Potential Problems</a></li>
    <li><a href="#iv-comparison-to-k-nearest-neighbors-regression">IV. Comparison to K-Nearest Neighbors Regression</a></li>
    <li><a href="#v-applications-in-r">V. Applications in R</a></li>
  </ul>

</div>

<h3 id="i-simple-linear-regression">I. Simple Linear Regression</h3>

<p><span class="boxheader">The Basics</span> – simple linear regression takes the form:</p>

<script type="math/tex; mode=display">y = {\beta}_0 + {\beta}_1 x</script>

<p>To find the line that is closest to the data, we use the least squares method and <strong>minimize the residual sum of squares</strong> (RSS), the amount of unexplained variability in the response after the regression:</p>

<script type="math/tex; mode=display">RSS = \sum_{i = 1} ^ n (y_i - \hat {y}_i) ^ 2</script>

<p>Solving for the coefficient estimates <script type="math/tex">\hat {\beta}_1</script> and <script type="math/tex">\hat {\beta}_0</script>, we find that:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat {\beta}_1 & = \frac { \sum_{i = 1}^n (x_i - \bar x) (y_i - \bar y)} {\sum_{i = 1}^n (x_i - \bar x)}\\
\hat {\beta}_0 & = \bar y - \hat {\beta}_1 \bar x
\end{align} %]]></script>

<ul>
  <li>Note: we can also prove that these estimates are unbiased, i.e. <script type="math/tex">E(\hat {\beta}_0) = \beta_0</script> and <script type="math/tex">E(\hat {\beta}_1) = \beta_1</script></li>
</ul>

<p><span class="boxheader">Measuring accuracy of the coefficient estimates </span> – using the general variance formula, <script type="math/tex">Var (\hat u) = SE ( \hat u ) ^ 2 = \frac {\sigma ^ 2} {n}</script>, we can solve for the <strong>standard error of the least squares coefficients</strong>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
SE ( \hat {\beta}_1 ) ^ 2 & = \frac{ \sigma ^ 2} { \sum_{i = 1}^n (x_i - \bar x) ^ 2} \\
SE ( \hat {\beta}_0 ) ^ 2 & = \sigma ^ 2 \Biggl[ \frac {1} {n} + \frac{\bar x ^ 2} {\sum_{i = 1}^n (x_i - \bar x) ^ 2} \Biggr]
\end{align} %]]></script>

<ul>
  <li>Note: <script type="math/tex">\hat \sigma ^ 2</script>, the <strong>residual standard error</strong> (RSE) is given by <script type="math/tex">\sqrt {RSS / (n - 2)}</script>.</li>
</ul>

<p>These standard errors can be used to compute <strong>confidence intervals</strong>; for example, <script type="math/tex">\hat {\beta}_1 \pm z_{\alpha / 2} * SE(\hat {\beta}_1)</script>.  A 95% confidence interval means that if repeated samples were taken and 95% confidence intervals were computed for each sample, 95% of those intervals are expected to contain the population mean.</p>

<p>To perform <strong>hypothesis testing</strong>, we compute a t-statistic, given by <script type="math/tex">\frac{\hat {\beta}_1}{SE(\hat {\beta}_1)}</script>. These have an associated p-value, which represents the probability, assuming the null hypothesis is true, that we received the observed sample results.</p>

<p><span class="boxheader">Measuring accuracy of the model </span></p>

<ol>
  <li>The residual standard error (RSE) is a measure of the lack of fit of the model in units of the response</li>
  <li>The <script type="math/tex">R^2</script> statistic is the proportion of variability in Y that can be explained using X, and is calculated with the formula:</li>
</ol>

<script type="math/tex; mode=display">R ^ 2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}</script>

<p>where the total sum of squares (TSS), given by <script type="math/tex">\sum(y_i - \bar y) ^ 2</script>, measures the total variance in the response before the regression.</p>

<ul>
  <li>Note: The <script type="math/tex">R^2</script> statistic is equal to the correlation between X and Y in simple linear regression with one predictor.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="ii-multiple-linear-regression">II. Multiple Linear Regression</h3>

<p><span class="boxheader">The Basics</span> – multiple linear regression takes the form:</p>

<script type="math/tex; mode=display">Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p</script>

<p>Again, the least squares solution involves minimizing the residual sum of squares (RSS).</p>

<p><span class="boxheader">Is there a relationship between the response and predictors? </span></p>

<p>We can use a hypothesis test to investigate whether all of the regression coefficients are zero using an F-statistic:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
H_0 & : \beta_1 = \beta_2 = ... = \beta_p = 0 \\ 
F & = \frac{(TSS - RSS) / p}{RSS / (n - p - 1)}
\end{align} %]]></script>

<p>If linear model assumptions are correct, then <script type="math/tex">E[RSS / (n - p - 1)] = \sigma ^ 2</script>, and if there is no relationship between the response and predictors, then <script type="math/tex">E[(TSS - RSS) / p] = \sigma ^ 2</script>. Therefore, when there is no relationship between the response and predictors, F-statistic will take on a value close to 1, but when there is a relationship, the F-statistic will be greater than 1.</p>

<ul>
  <li>Note: The smaller the sample size n, the larger the F-statistic needs to be to reject the null hypothesis.</li>
</ul>

<p>On the other hand, if you want to test if a particular subset of <em>q</em> coefficients are zero, then fit a second model that uses all of the variables except the last <em>q</em>, and note the residual sum of squares for that model, <script type="math/tex">RSS_0</script>. Then the null hypothesis and F-statistic are:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
H_0 & : \beta_{p - q + 1} = \beta_{p - q + 2} = ... = \beta_p = 0 \\
F & = \frac{(RSS_0 - RSS) / q}{RSS / (n - p - 1)}
\end{align} %]]></script>

<p><span class="boxheader">How do we decide on important variables? </span></p>

<p>Variables can be selected through three classical approaches:</p>
<ul>
  <li>Forward selection
    <ul>
      <li>Start with the null model, fit new models with each of the <em>p</em> predictors, then add the variable that resulted in the lowest RSS. Re-fit the base model, fit new models with remaining predictors, and repeat until a stopping condition.</li>
    </ul>
  </li>
  <li>Backward selection
    <ul>
      <li>Start with all variables in the model, and remove the variable with the largest p-value. Re-fit the model, and repeat until a stopping condition.</li>
      <li>Cannot be used if <script type="math/tex">p > n</script></li>
    </ul>
  </li>
  <li>Mixed selection
    <ul>
      <li>Start with no variables in the model. Add variables that provide the best fit, but remove variables if the p-value for any variable rises above a certain threshold.</li>
    </ul>
  </li>
</ul>

<p><span class="boxheader">How well does the model fit the data? </span></p>

<ol>
  <li>For multiple linear regression, <script type="math/tex">R^2 = Cor (Y, \hat Y) ^ 2</script> . Note that <script type="math/tex">R^2</script> will always increase for the training data set when adding more variables, so be sure to test the model on a testing data set.</li>
  <li>Residual standard error is defined generally as: <script type="math/tex">\sqrt { \frac {RSS} {n - p - 1}}</script></li>
</ol>

<p>The quality of a model can be judged by:</p>
<ul>
  <li>Mallow’s <script type="math/tex">C_p</script></li>
  <li>AIC (Akaike information Criterian)</li>
  <li>BIC (Bayesian information criterion)</li>
  <li>Adjusted <script type="math/tex">R^2</script></li>
</ul>

<p>To compare two models, we can perform ANOVA (analysis of variance) using an F-test in order to test the null hypothesis that a model <script type="math/tex">M_1</script> is sufficient to explain the data against the alternative hypothesis that a more complex model <script type="math/tex">M_2</script> is required. These must be nested models: the predictors in <script type="math/tex">M_1</script> must be a subset of the predictors in <script type="math/tex">M_2</script>.</p>

<p><span class="boxheader">How accurate are our model predictions?</span></p>

<p>There are three types of uncertainty associated with prediction:</p>

<ol>
  <li>
    <p>The least squares plane, <script type="math/tex">\hat Y = \hat \beta_0 + \hat \beta_1 x_1 + \hat \beta_2 x_2 + ... + \hat \beta_p x_p</script>,  is only an estimate for the true population regression plane, <script type="math/tex">Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p</script>
. This inaccuracy in the coefficient estimates is related to the <strong>reducible error</strong> of the model, and we can estimate this uncertainty with a <strong>confidence interval.</strong></p>
  </li>
  <li>
    <p>Another source of <strong>reducible error</strong> comes from the <strong>model bias</strong>, since we are estimating the best linear approximation to the true surface, which may not be linear.</p>
  </li>
  <li>
    <p>Even if we knew <script type="math/tex">f(X)</script>, the response value can’t be predicted perfectly because of the random error, <script type="math/tex">\epsilon</script> in the model. This is the <strong>irreducible error</strong>. We can estimate how much <script type="math/tex">Y</script> will vary from <script type="math/tex">\hat Y</script> with <strong>prediction intervals</strong>, which are always wider than confidence intervals because they incorporate both the error in the estimate for <script type="math/tex">f(X)</script> and the uncertainty as to how much an individual point will differ from the population regression plane.</p>
  </li>
</ol>

<hr />
<hr />
<hr />
<h3 id="iii-considerations-and-potential-problems">III. Considerations and Potential Problems</h3>

<p><span class="boxheader">Considerations</span></p>

<ul>
  <li>Qualitative Predictors
    <ul>
      <li>For predictors with two values/levels, we use dummy variable that takes the value of either 1 or 0. Note that the assigned values will affect interpretation.</li>
      <li>For predictors with more than two values, we can create additional dummy variables (a total of 1 less than the number of levels).</li>
      <li>We can also code qualitative variables in other ways to measure particular contrasts between variables, which will lead to equivalent models with different coefficients and interpretations.</li>
    </ul>
  </li>
  <li>Extensions of the Linear Model
    <ul>
      <li>Linear regression assumes that the relationship between response and predictor is additive and linear.
        <ul>
          <li><strong>Non-additive relationships</strong> can be representated using an <strong>interaction effect</strong> in addition to the main effects. Be sure to abide by the hierarchical principle – if using an interaction term, always include the main effects as well since <script type="math/tex">X_1 * X_2</script> is often correlated with <script type="math/tex">X_1</script> and <script type="math/tex">X_2</script>.</li>
          <li><strong>Non-linear relationships</strong> can be represented using <strong>polynomial regression</strong>, among other methods.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><span class="boxheader">Potential Problems</span></p>

<ol>
  <li>Non-linearity of the repsonse-predictor relationships
    <ul>
      <li>check the <strong>residual plots</strong> of <script type="math/tex">e_i = y_i - \hat y_i</script> against <script type="math/tex">x_i</script> to make sure the errors are approximately random</li>
      <li>if residuals exhibit any trends, then try using non-linear transformations of predictors, such as <script type="math/tex">\log x, \sqrt x, x^2</script></li>
    </ul>
  </li>
  <li>Correlation of error terms
    <ul>
      <li><script type="math/tex">\epsilon_i</script> should provide no information about <script type="math/tex">\epsilon_{i + 1}</script>, otherwise the sample standard error will be an underestimate of the true standard error, a confidence interval will not be wide enough, and p-values will be lower than they should be.</li>
      <li>in time series data, <strong>residuals plotted against time</strong> may exhibit tracking if adjacent residuals are similar; there are many methods (not shown here) that can be used to address this</li>
      <li>outside of time-series data, good experimental design can mitigate the risk of correlated errors</li>
    </ul>
  </li>
  <li>Non-constant variance of error terms
    <ul>
      <li>heteroskedasticity, or a non-constant <script type="math/tex">\sigma^2</script>, looks like a <strong>funnel-shape in residual plots</strong></li>
      <li>potential solution: try transforming the response variable with <script type="math/tex">\log Y</script> or <script type="math/tex">\sqrt Y</script></li>
      <li>potential solution: weighted least squares can solve this; for example, if the <script type="math/tex">i</script>th response is an average of <script type="math/tex">n_i</script> raw observations that are uncorrelated with their variance, then their average has variance <script type="math/tex">\sigma_i^2 = \sigma^2/n_i</script> and the non-constant variance is solved by using weighted least squares with observation weights <script type="math/tex">w_i = n_i</script></li>
    </ul>
  </li>
  <li>Outliers
    <ul>
      <li>outliers that can affect the model, standard error, confidence intervals, and <script type="math/tex">R^2</script></li>
      <li>check for outliers with <strong>studentized residual plots</strong>, where residuals are scaled by the standard error <script type="math/tex">\bigl( \frac {e_i}{\text{Std Err}} \bigr)</script>. If studentized residuals are greater than 3, then the observations are potential outliers</li>
    </ul>
  </li>
  <li>High-leverage Points
    <ul>
      <li>points with an unusual x-value are high-leverage points that can adversely affect the model</li>
      <li>leverage can be quantified with the <strong>leverage statistic</strong><sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>; this is always between <script type="math/tex">\frac {1} {n}</script> and 1, and the average is <script type="math/tex">\frac {p + 1} {n}</script></li>
      <li>if a given observation greatly exceeds <script type="math/tex">(p + 1) / n</script>, then the corresponding point may have high leverage</li>
    </ul>
  </li>
  <li>Collinearity
    <ul>
      <li>when two or more predictors are highly correlated, it reduces the accuracy of estimates – the standard error of <script type="math/tex">\beta_j</script> increases, t-statistic decreases, and the power (chance of correctly determining a non-zero coefficient) of the hypothesis test goes down</li>
      <li>potential solutions include dropping or combining collinear variables</li>
      <li>use a correlation matrix to detect collinearity between two variables</li>
      <li>use the <strong>variance inflation factor (VIF)</strong>, shown below, to assess multicollinearity
        <ul>
          <li><script type="math/tex">R_{x_j \vert x_{-j}}^2</script> is the <script type="math/tex">R^2</script> of the regression of <script type="math/tex">X_j</script> onto all other predictors.</li>
          <li>Note that the smallest value of 1 indicates no collinearity; if it exceeds 5 or 10, then it may be a problem.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
VIF & = \frac {Var(\hat \beta_j, \text{full model})} {\hat \beta_j, \text{fit on its own}}
    & = \frac {1} {1 - R_{x_j \vert x_{-j}}^2}
\end{align} %]]></script>

<hr />
<hr />
<hr />
<h3 id="iv-comparison-to-k-nearest-neighbors-regression">IV. Comparison to K-Nearest Neighbors Regression</h3>

<p>K-Nearest-Neighbors regression takes the form:</p>

<script type="math/tex; mode=display">\hat {f}(x_o) = \frac {1}{k} \sum_{x_i \in N_0} y_i</script>

<p>In other words, given a value for K and a prediction point <script type="math/tex">x_0</script>, KNN regression identifies the <script type="math/tex">K</script> training observations that are closest to <script type="math/tex">x_0</script>, represented by <script type="math/tex">N</script>, and estimates <script type="math/tex">f(x_o)</script> using the average of all the training responses in <script type="math/tex">N_0</script>.</p>

<ul>
  <li>The optimal K depends on the variance-bias tradeoff; a bigger K leads to a smoother, less flexible fit with low variance and high bias while a smaller K leads to a bumpier fit with high variance and low bias.</li>
  <li>A parametric approach will outperform a non-parametric form if it is close to the true form of <script type="math/tex">f</script>, or when there is a small number of observations per predictor.</li>
  <li>KNN Regression suffers from the curse of dimensionality:
    <ul>
      <li>more predictors (dimensions) means neighbors are farther from each other</li>
      <li>KNN performs poorly with lots of predictors</li>
      <li>in higher dimensions, KNN often performs worse than linear regression</li>
    </ul>
  </li>
</ul>

<hr />
<hr />
<hr />
<h3 id="v-applications-in-r">V. Applications in R</h3>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="s2">"MASS"</span><span class="p">)</span><span class="w"> </span><span class="c1"># lots of functions and data sets</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">)</span><span class="w">

</span><span class="n">confint</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.95</span><span class="p">)</span><span class="w"> </span><span class="c1"># 95% confidence interval</span><span class="w">

</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="w"> </span><span class="c1"># predicted values</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">),</span><span class="w"> </span><span class="n">residuals</span><span class="p">(</span><span class="n">model</span><span class="p">))</span><span class="w"> </span><span class="c1"># plot residuals</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">),</span><span class="w"> </span><span class="n">rstudent</span><span class="p">(</span><span class="n">model</span><span class="p">))</span><span class="w"> </span><span class="c1"># plot studentized residuals</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">hatvalues</span><span class="p">(</span><span class="n">model</span><span class="p">))</span><span class="w"> </span><span class="c1"># calculate and plot leverage statistics</span><span class="w">
</span><span class="n">which.max</span><span class="p">(</span><span class="n">hatvalues</span><span class="p">(</span><span class="n">model</span><span class="p">))</span><span class="w"> </span><span class="c1"># display the index of the observation with largest leverage</span><span class="w">

</span><span class="n">vif</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="w"> </span><span class="c1"># calculates variance inflation factors</span><span class="w">

</span><span class="n">anova</span><span class="p">(</span><span class="n">model1</span><span class="p">,</span><span class="w"> </span><span class="n">model2</span><span class="p">)</span><span class="w"> </span><span class="c1"># compare two nested models</span><span class="w">
</span><span class="n">contrasts</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"> </span><span class="n">displays</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">levels</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">factor</span></code></pre></figure>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simple linear regression, the leverage statistic is given by <script type="math/tex">h_i = \frac {1} {n} + \frac{(x_i - \bar x) ^ 2} {\sum_{i = 1}^n (x_{i'}) - \bar x)}</script>. <a href="#fnref:1" class="reversefootnote">⤴</a></p>
    </li>
  </ol>
</div>

          </div>
          <!--
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Linear+Regression%20-%20http://localhost:4000/posts/linear-regression" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/linear-regression" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>
          -->
          <span class="backbutton"><a href="/posts/index">←Index</a></span>
          
        </article>
        <footer class="footer ">

<small class="pull-left"> &copy;2020 All rights reserved. 

<a href="https://github.com/nielsenramon/chalk" target="_blank">Chalk</a> theme by Nielson Ramon.

</small>
</footer>

      </div>
    </div>
  </main>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160038909-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-160038909-1');
  </script>


<script src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>




<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }
   });
   MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
     var TEX = MathJax.InputJax.TeX;
     var COLS = function (W) {
       var WW = [];
       for (var i = 0, m = W.length; i < m; i++)
         {WW[i] = TEX.Parse.prototype.Em(W[i])}
       return WW.join(" ");
     };
     TEX.Definitions.Add({
       environment: {
         psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
       }
     });
   });
 </script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
