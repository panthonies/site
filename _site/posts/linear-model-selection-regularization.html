<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anthony Pan | Linear Model Selection and Regularization</title>
  <meta name="description" content="An alternative fitting method to least squares, such as subset selection, shrinkage (ridge regression, lasso), and dimension reduction techniques (principle components analysis, partial least squares) can help prediction accuracy and model interpretability.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Linear Model Selection and Regularization">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/posts/linear-model-selection-regularization">
  <meta property="og:description" content="An alternative fitting method to least squares, such as subset selection, shrinkage (ridge regression, lasso), and dimension reduction techniques (principle components analysis, partial least squares) can help prediction accuracy and model interpretability.">
  <meta property="og:site_name" content="Anthony Pan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://localhost:4000/posts/linear-model-selection-regularization">
  <meta name="twitter:title" content="Linear Model Selection and Regularization">
  <meta name="twitter:description" content="An alternative fitting method to least squares, such as subset selection, shrinkage (ridge regression, lasso), and dimension reduction techniques (principle components analysis, partial least squares) can help prediction accuracy and model interpretability.">

  
    <meta property="og:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
    <meta name="twitter:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
  

  <link href="http://localhost:4000/feed.xml" type="application/rss+xml" rel="alternate" title="Anthony Pan Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-3ebe45100a3d97f8ac94857224f3fde7193d453226ebbb900022771bfa032719.css">
    

  

</head>
<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav ">
  <a href="/" class="header-logo" title="Anthony Pan">Anthony Pan</a>
  <ul class="header-links">
    
      <li>
        <a href="/" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="/posts/index" title="Blog">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-writing">
  <use href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing" xlink:href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article ">
          <header class="article-header">
            <h1>Linear Model Selection and Regularization</h1>
            <p>An alternative fitting method to least squares, such as subset selection, shrinkage (ridge regression, lasso), and dimension reduction techniques (principle components analysis, partial least squares) can help prediction accuracy and model interpretability.</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    April 27, 2020
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      20 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
      
      <a href="/tag/academic" title="See all posts with tag 'academic'">academic</a>
    
  </div>
</div>
          </header>

          <div class="article-content">
            <div class="toc">
  <h3 id="contents">Contents</h3>

  <ul>
    <li><a href="#i-overview">I. Overview</a></li>
    <li><a href="#ii-subset-selection">II. Subset Selection</a>
      <ul class="hi"><li>Best subset selection</li>
      <li>Forward stepwise selection</li>
      <li>Backward stepwise selection</li>
      <li>Statistics for model selection</li></ul></li>
    <li><a href="#iii-shrinkage">III. Shrinkage</a>
      <ul class="hi"><li>Ridge Regression</li>
      <li>Lasso</li></ul></li>
    <li><a href="#iv-dimension-reduction-techniques">IV. Dimension Reduction Techniques</a>
      <ul class="hi"><li>Principle components analysis</li>
      <li>Partial least squares</li></ul></li>
    <li><a href="#v-applications-in-r">V. Applications in R</a></li>
  </ul>
</div>

<h3 id="i-overview">I. Overview</h3>

<p>Why use an alternative fitting model to least squares?</p>

<ul>
  <li>Prediction Accuracy
    <ul>
      <li>If <script type="math/tex">n</script> is not much larger than <script type="math/tex">p</script>, then least squares can have a lot of variability</li>
      <li>If <script type="math/tex">% <![CDATA[
p < n %]]></script>, then there is no longer a unique least squares coefficient estimate, and we must constrain or shrink the coefficients</li>
    </ul>
  </li>
  <li>Model Interpretability
    <ul>
      <li>Remove irrelevant variables with feature/variable selection</li>
    </ul>
  </li>
</ul>

<p><span class="boxheader">Alternatives to Least Squares</span></p>

<ol>
  <li>Subset selection: select a subset of predictors before fitting least squares</li>
  <li>Shrinkage: fit model with all predictors, with coefficients shrunk toward 0 relative to the least squares estimate</li>
  <li>Dimension reduction: projecting predictors onto an <script type="math/tex">M</script>-dimensional sbspace where <script type="math/tex">% <![CDATA[
M < p %]]></script> by computing projections/linear combinations of the variables.  These <script type="math/tex">M</script> projections are then used as predictors for least squares.</li>
</ol>

<hr />
<hr />
<hr />
<h3 id="ii-subset-selection">II. Subset Selection</h3>

<p><span class="boxheader">Best Subset Selection</span></p>

<ol>
  <li>Let <script type="math/tex">M_0</script> denote the null model.</li>
  <li>For <script type="math/tex">k = 1, 2, ..., p</script>, fit all models with <script type="math/tex">k</script> predictors, and choose the best one (smallest RSS/largest <script type="math/tex">R^2</script>), <script type="math/tex">M_k</script>.</li>
  <li>Select the best model from <script type="math/tex">M_1, ..., M_k</script> using a model selection metric such as adjusted <script type="math/tex">R^2</script>, AIC, BIC, or cross-validated prediction error.</li>
</ol>

<p>Note: this is very computationally expensive, and infeasible for <script type="math/tex">p > 40</script>, since there are <script type="math/tex">2^p</script> total models to check. In addition this method has high variance which can lead to overfitting.</p>

<p><span class="boxheader">Forward Stepwise Selection</span></p>

<ol>
  <li>Let <script type="math/tex">M_0</script> denote the null model.</li>
  <li>For <script type="math/tex">k = 0, 2, ..., p - 1</script>, consider all <script type="math/tex">p - k</script> models that add one additional predictor to <script type="math/tex">M_k</script>, then choose the best one, <script type="math/tex">M_{k+1}</script>.</li>
  <li>Select the best model from <script type="math/tex">M_0, ..., M_p</script> using a model selection metric.</li>
</ol>

<p>Note: this is much more computationally feasible than best subset selection with <script type="math/tex">1 + \frac{p(p+1)}{2}</script> total models to check, but it’s not guaranteed to yield the “best” model for the training data. When <script type="math/tex">p>n</script>, this is the only feasible subset selection method.</p>

<p><span class="boxheader">Backward Stepwise Selection</span></p>

<ol>
  <li>Let <script type="math/tex">M_0</script> denote the null model.</li>
  <li>For <script type="math/tex">k = p, p-1, ..., 1</script>, consider all <script type="math/tex">k</script> models that contain all but 1 of the predictors in <script type="math/tex">M_k</script>, a total of <script type="math/tex">k-1</script> predictors, then choose the best one, <script type="math/tex">M_{k+1}</script>.</li>
  <li>Select the best model from <script type="math/tex">M_0, ..., M_p</script> using a model selection metric.</li>
</ol>

<p>Note: Like forward selection, this only searches through <script type="math/tex">1 + \frac{p(p+1)}{2}</script> models, and is not guaranteed to yield the best model. It cannot be used with <script type="math/tex">p>n</script>. Hybrid approaches (forward-backward) may be used as well.</p>

<p><span class="boxheader">Model selection statistics</span></p>

<p>We can choose the optimal model either by estimate the test error <em>indirectly</em> by adjusting the training error to account for overfitting bias, or <em>directly</em> by using a validation set or cross-validation.</p>

<p><strong>Adjusting the training error:</strong></p>
<ul>
  <li><script type="math/tex">C_p = \frac{1}{n} (\text{RSS} + 2 d \hat \sigma^2)</script> ;
    <ul>
      <li>Penalizes the model based on the number of predictors, <script type="math/tex">d</script>. Lower is better.</li>
      <li>If <script type="math/tex">\hat \sigma^2</script> is an unbiased estimator of <script type="math/tex">\sigma^2</script>, then <script type="math/tex">C_p</script> is an unbiased estimator of the test MSE.</li>
    </ul>
  </li>
  <li><script type="math/tex">\text{AIC} = \frac{1}{n \hat \sigma^2} (\text{RSS} + 2 d \hat \sigma^2)</script> ;
    <ul>
      <li>Use this for models fit by maximum likelihood.</li>
      <li>With gaussian errors, maximum likelihood = least squares.</li>
    </ul>
  </li>
  <li><script type="math/tex">\text{BIC} = \frac{1}{n \hat \sigma^2} (\text{RSS} + \log {n} d \hat \sigma^2)</script> ;
    <ul>
      <li>Use this for least squares model with <script type="math/tex">d</script> predictors.</li>
      <li>Has a heavier penalty for models with many variables.</li>
    </ul>
  </li>
  <li><script type="math/tex">\text{Adj. } R^2 = 1 - \frac{\text{RSS} / (n - d - 1)} {TSS / (n-1)}</script> ;
    <ul>
      <li>Larger is better.</li>
    </ul>
  </li>
</ul>

<p>Note: These formulas are all for linear least squares, and can be generalized for additional model types.
m
<strong>Cross Validation or validation set error:</strong></p>
<ul>
  <li>Makes fewer assumptions about the underlying model, and can be used when the number of predictors is unknown, or <script type="math/tex">\hat \sigma^2</script> cannot be estimated.</li>
  <li>One Standard Error Rule: choose the smallest model for which the estimated test MSE is within one standard error of the lowest point on the curve.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="iii-shrinkage">III. Shrinkage</h3>

<p>Shrinking coefficient estimates can significantly reduce model variance.</p>

<p><span class="boxheader">Ridge Regression</span></p>

<p>While least squares minimizes RSS, ridge regression picks coefficient estimates <script type="math/tex">\hat \beta^R</script> to minimize:</p>

<script type="math/tex; mode=display">\text{RSS} + \lambda \sum_{j = 1}^p \beta_j^2</script>

<p>Where:</p>
<ul>
  <li><script type="math/tex">\text{RSS} = \sum_{i = 1}^n \Bigl( _i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \Bigr) ^ 2</script> ;</li>
  <li><script type="math/tex">\lambda \geq 0</script> is a tuning parameter</li>
  <li><script type="math/tex">\lambda \sum_j \beta_j^2</script> is the shrinkage penalty, and grows as <script type="math/tex">\lambda \to \infty</script></li>
</ul>

<p>It is best to perform ridge regression after standardizing the predictors (note the denomenator is the standard deviation of the <script type="math/tex">j</script>th predictor):</p>

<script type="math/tex; mode=display">\tilde x_{ij} = \frac{x_{ij}} {\sqrt {\frac {1} {n} \sum_{i = 1}^n (x_{ij} - \bar x_j)^2}}</script>

<p><strong>Improvements over least squares:</strong></p>
<ul>
  <li>Bias-variance tradeoff: as <script type="math/tex">\lambda</script> increases, variance decreases and bias increases.</li>
  <li>In cases where relationships between predictors and response is approximate linear, then least squares will have low bias but may have high variance, in particular when the number of variables is almost as big, or bigger, than the sample size.</li>
  <li>Ridge regression works best when the least squares estimate has high variance.</li>
  <li>Computationally superior to best subset selection, since solving for all <script type="math/tex">\lambda</script> is almost computationally equivalent to fitting least squares.</li>
</ul>

<p><strong>Drawback:</strong></p>
<ul>
  <li>Ridge regression will include all <script type="math/tex">p</script> predictors in the final model, which presents challenges for interpretability.</li>
</ul>

<p><span class="boxheader">Lasso</span></p>

<p>The lasso coefficients <script type="math/tex">\hat \beta_{\lambda}^L</script> minimize the equation:</p>

<script type="math/tex; mode=display">\text{RSS} + \lambda \sum_{j = 1}^p \vert \beta_j \vert</script>

<p>Where <script type="math/tex">\text{RSS } = \sum_{i = 1}^n \Bigl( _i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \Bigr) ^ 2</script></p>

<p><strong>Notes:</strong></p>
<ul>
  <li>Bias-variance tradeoff: as <script type="math/tex">\lambda</script> increases, variance decreases and bias increases.</li>
  <li>Lasso uses an <script type="math/tex">l_1</script> penalty instead of  the <script type="math/tex">l_2</script> penalty of ridge regression.</li>
  <li>The <script type="math/tex">l_1</script> norm of coefficient vector <script type="math/tex">\beta</script> is <script type="math/tex">\lVert \beta \rVert _1 = \sum \vert \beta_j \vert</script>.</li>
  <li>Performs variable selection by forcing some coefficients to 0, and yields sparse models.</li>
</ul>

<p><span class="boxheader">Comparison of Ridge Regression and Lasso</span></p>

<p><strong>Ridge Regression:</strong></p>
<ul>
  <li><script type="math/tex">\min_{\beta} \text{\{RSS\}} \text{ subject to } \sum_{j = 1}^p \beta_j^2 \leq s</script> ;</li>
  <li>uses <script type="math/tex">l_2</script> penalty</li>
  <li>computationally feasible alternative to best subset selection</li>
  <li>outperforms lasso when all variables are related to the response, or when there are many predictors of relatively equal size.</li>
</ul>

<p><strong>Lasso:</strong></p>
<ul>
  <li><script type="math/tex">\min_{\beta} \text{\{RSS\}} \text{ subject to } \sum_{j = 1}^p \vert \beta_j \vert \leq s</script> ;</li>
  <li>uses <script type="math/tex">l_1</script> penalty</li>
  <li>computationally feasible alternative to best subset selection</li>
  <li>outperforms ridge when only a few predictors have substantial coefficients, and the rest are very small or 0. It also has advantages in that it performs variable selection and is easier to interpret.</li>
</ul>

<p>The tuning parameter <script type="math/tex">\lambda</script> should be selected with cross validation by selecting a grid of <script type="math/tex">\lambda</script> values, computing the cross-validation error for each <script type="math/tex">\lambda</script>, and selecting <script type="math/tex">\lambda</script> for which cv error is smallest.</p>

<p>The classic image below<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>  illustrates the difference between ridge regression and lasso in two dimensions. Points along each red ellipse have equal RSS, and the blue areas represent the <script type="math/tex">l_1</script> and <script type="math/tex">l_2</script> constraints. The ridge and lasso solutions are where the red ellipses touch the blue areas.</p>

<center><a href="/assets/04-27-lasso-ridge-918624ebc1cf36eafb918091343069d35ba98f142c9fe628d178fb23a39ddd55.png">
  <img src="/assets/04-27-lasso-ridge-918624ebc1cf36eafb918091343069d35ba98f142c9fe628d178fb23a39ddd55.png" alt="Lasso vs Ridge" class="zooming" data-rjs="/assets/04-27-lasso-ridge-918624ebc1cf36eafb918091343069d35ba98f142c9fe628d178fb23a39ddd55.png" data-zooming-width="" data-zooming-height="" />
</a>
</center>

<p>Further intuition for a simple case is given below<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>  – we see that ridge regression shrinks every dimension of data by the same proportion, while lasso shrinks all coefficients toward 0 by a similar amount, and sufficiently small coefficients are shrunk to 0.</p>

<center><a href="/assets/04-27-lasso-ridge-2-2a5a61e080a82d1e706d8d4b4762e663bba0476411379ef442d1a2642548e409.png">
  <img src="/assets/04-27-lasso-ridge-2-2a5a61e080a82d1e706d8d4b4762e663bba0476411379ef442d1a2642548e409.png" alt="Lasso vs Ridge 2" class="zooming" data-rjs="/assets/04-27-lasso-ridge-2-2a5a61e080a82d1e706d8d4b4762e663bba0476411379ef442d1a2642548e409.png" data-zooming-width="" data-zooming-height="" />
</a>
</center>

<p><span class="boxheader">Extra Theory: a bayesian interpretation for ridge regression and lasso</span></p>

<p>Suppose that:</p>
<ul>
  <li>a coefficient vector <script type="math/tex">\beta</script> has a prior distribution <script type="math/tex">p(\beta)</script>, where <script type="math/tex">\beta = (\beta_0, ..., \beta_p)</script></li>
  <li>the likelihood of the data can be written as <script type="math/tex">f(Y \vert X, \beta)</script>, where <script type="math/tex">X = (X_1, ..., X_p)</script></li>
</ul>

<p>Then, multipling the prior distribution by the likelihood gives the posterior distribution up to a proportionality constant:</p>

<script type="math/tex; mode=display">P(\beta \vert X, Y) \propto f(Y \vert X, \beta) p(\beta \vert X) = f(Y \vert X, \beta) p(\beta)</script>

<p>Assume:</p>
<ul>
  <li><script type="math/tex">Y = \beta_0 + X_1 \beta_1 + ... + X_p \beta_p + \epsilon</script> ; where <script type="math/tex">\epsilon \sim \text{iid} N(\mu, \sigma)</script></li>
  <li><script type="math/tex">p(\beta) = \Pi_{j = 1}^P g(\beta_j)</script> for some density function <script type="math/tex">g</script></li>
</ul>

<p>Then,</p>
<ul>
  <li>If <script type="math/tex">g</script> is a <strong>gaussian distribution</strong> with mean 0 and standard deviation that is a function of <script type="math/tex">\lambda</script>, then the posterior mode (most likely value for <script type="math/tex">\beta</script> given the data) for <script type="math/tex">\beta</script> is given by the <strong>ridge regression solution</strong>.</li>
  <li>If <script type="math/tex">g</script> is a <strong>double-exponential (Laplace) distribution</strong> with mean 0 and scale that is a function of <script type="math/tex">\lambda</script>, then the posterior mode for <script type="math/tex">\beta</script> is given by the <strong>lasso solution</strong>.
    <ul>
      <li>However, note that the lasso solution is not the posterior mean, and the posterior mean doesn’t yield a sparse coefficient vector.</li>
    </ul>
  </li>
</ul>

<p>A visualization of the gaussian and double-exponential distributions:<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup></p>

<center><a href="/assets/04-27-lasso-ridge-3-d5f6470b683ec467c2e13a117cf69e5b93e535be514f64113803253b3684a899.png">
  <img src="/assets/04-27-lasso-ridge-3-d5f6470b683ec467c2e13a117cf69e5b93e535be514f64113803253b3684a899.png" alt="Lasso vs Ridge 3" class="zooming" data-rjs="/assets/04-27-lasso-ridge-3-d5f6470b683ec467c2e13a117cf69e5b93e535be514f64113803253b3684a899.png" data-zooming-width="" data-zooming-height="" />
</a>
</center>

<hr />
<hr />
<hr />
<h3 id="iv-dimension-reduction-techniques">IV. Dimension Reduction Techniques</h3>

<p>These techniques transform the predictors, before using least squares to fit a model with the transformed predictors. When the number of predictors is large relative to the sample size, then significantly reducing the number of dimensions can significantly reduce variance. In more formal terms…</p>

<p>Let <script type="math/tex">Z_1, ..., Z_m</script> represent <script type="math/tex">% <![CDATA[
M < p %]]></script> linear combinations of the original <script type="math/tex">p</script> predictors:</p>

<script type="math/tex; mode=display">Z_m = \sum_{j = 1}^p \phi{jm} X_j \text {  for some constants  } \theta_{1m}, ..., \theta_{pm}, m = 1, ..., M</script>

<p>Then using least squares, fit the linear regression model: <script type="math/tex">y_i = \theta_0 + \sum_{m = 1}^M \theta_m z_{im} + \epsilon_i</script></p>
<ul>
  <li>The regression coefficients are <script type="math/tex">\theta_0, ..., \theta_m</script></li>
  <li>Dimensions are reduced from <script type="math/tex">p+1</script> to <script type="math/tex">M+1</script></li>
  <li>Coefficients are restrained by <script type="math/tex">\beta_j = \sum_{i = 1}^M \theta \phi_{jm}</script></li>
</ul>

<p><span class="boxheader">Principle Components Analysis</span></p>

<p>The first principle components vector is chosen from all combinations of predictors such that:</p>
<ul>
  <li><script type="math/tex">\phi_{11}^2 + ... + \phi_{pm}^2 = 1</script> ;</li>
  <li>captures the highest variance of predictors</li>
  <li>defines the line that is as close as possible to the data</li>
  <li>the first principle components score for the <script type="math/tex">i</script>th observation is the distance in the <script type="math/tex">x</script>-direction of the <script type="math/tex">i</script>th projection from 0, the mean point of all predictors.</li>
  <li>all principle components vectors are orthogonal to each other</li>
</ul>

<p>The second principle component <script type="math/tex">Z_2</script> is a linear combination of the variables that is uncorrelated with <script type="math/tex">Z_1</script> and has the largest variance, which means it must be orthogonal to the first principal component direction. Other principle components can be calculated in the same way.</p>

<p><strong>Principle components regression</strong></p>
<ul>
  <li>assume that the directions in which <script type="math/tex">X_1, ..., X_p</script> show the most variation are the directions that are associated with <script type="math/tex">Y</script></li>
  <li>using more principle components means lower bias, higher variance</li>
  <li>this is NOT a feature selection method, since it combines all <script type="math/tex">p</script> original features</li>
  <li>closely related to ridge regression</li>
</ul>

<p>Notes:</p>
<ul>
  <li>choose the number of principle components <script type="math/tex">M</script> using cross validation</li>
  <li>standardize each predictor before generating principle components, so that high variance variables don’t dominate</li>
</ul>

<p><span class="boxheader">Partial Least Squares</span></p>

<p>Partial Least Squares (PLS) is a supervised alternative to PCA that identifies new features <script type="math/tex">Z_1, ..., Z_m</script> that are combinations of the original predictors and <strong>related to the response</strong>. The biggest drawback of PCA is that its components are not guaranteed to be the ones that best explain the response, since it’s unsupervised.</p>

<ul>
  <li><strong>compute the first PLS direction</strong>, <script type="math/tex">Z_1 = \sum_{j = 1}^p \phi_{j1} X_j</script>, by setting each <script type="math/tex">\phi_{j1}</script> to the simple linear regression coefficients of <script type="math/tex">Y \sim X_j</script>.
    <ul>
      <li>this places the highest weights on variables most strongly correlated with the response when calculating <script type="math/tex">Z_1</script></li>
      <li>does not fit the predictors as well as PCA, but does a better job explaining the response</li>
    </ul>
  </li>
  <li><strong>compute the second PLS direction</strong>, <script type="math/tex">Z_2</script>, by regressing each variable on <script type="math/tex">Z_1</script> and taking the residuals, then compute <script type="math/tex">Z_2</script> by using the orthogonalized data in the same way as <script type="math/tex">Z_1</script>.</li>
  <li>Choose the number of predictors <script type="math/tex">M</script> with cross-validation.</li>
</ul>

<p>This method is popular in chemometrics, but in practice often performs no better than PCR or ridge regression. Relative to PCR, PLS has lower bias and higher variance.</p>

<p><span class="boxheader">Considerations in High Dimensions</span></p>

<p>What goes wrong when <script type="math/tex">p \geq n</script>?</p>
<ul>
  <li>be careful of overfitting – always evaluate perfromance on an independent test set</li>
  <li><script type="math/tex">C_p</script>, AIC, BIC cannot be used because estimating <script type="math/tex">\hat \sigma^2</script> is problematic. Adjusted <script type="math/tex">R^2</script> also can’t be used.</li>
</ul>

<p>Tips:</p>
<ul>
  <li>forward stepwise selection, ridge regression, lasso, and PCR are useful</li>
  <li>regularization/shrinkage plays a big role</li>
  <li>tuning parameter selection is very important</li>
  <li>test error will increase as dimensions increase unless additional features are truly predictive (curse of dimensionality)</li>
</ul>

<p>Interpreting results</p>
<ul>
  <li>In high dimensions, any variable can be written as a linear combination of the others. For this reason, we can’t know which variables are truly predictive of the outcome, or which coefficients are best.</li>
  <li>All models are only one of many possible models for prediction, and must be further validated on independent data sets.</li>
  <li>Never use the sum of squared errors, p-values, <script type="math/tex">R^2</script>, or others on training data as evidence of good fit. Instead, use either cross-validation results or test on an independent test set.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="v-applications-in-r">V. Applications in R</h3>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">### exercise!!!! </span><span class="w">
</span><span class="c1">### using COLLEGE data - predict Apps (# applications)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"ISLR"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"pls"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"glmnet"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"purrr"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"leaps"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"patchwork"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"ggplot2"</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># prepare data</span><span class="w">
</span><span class="n">college</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">College</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">split</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">College</span><span class="p">),</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">College</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">.7</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">college</span><span class="p">[</span><span class="n">split</span><span class="p">,]</span><span class="w">
</span><span class="n">test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">college</span><span class="p">[</span><span class="o">-</span><span class="n">split</span><span class="p">,]</span><span class="w">
</span><span class="n">train_matrix</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">
</span><span class="n">test_matrix</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">mean</span><span class="p">((</span><span class="n">pred</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">rsq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">((</span><span class="n">pred</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">((</span><span class="n">mean</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># least squares</span><span class="w">
</span><span class="n">model_lsq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">
</span><span class="n">pred_lsq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_lsq</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="p">(</span><span class="n">pred_lsq</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># MSE: 1261630</span><span class="w">
</span><span class="n">rsq</span><span class="p">(</span><span class="n">pred_lsq</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># R-Squared: .9135</span><span class="w">

</span><span class="c1"># least squares - best subset</span><span class="w">
</span><span class="n">model_lsq_best</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">regsubsets</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">nvmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">17</span><span class="p">)</span><span class="w">
</span><span class="n">model_lsq_best_summ</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">summary</span><span class="p">(</span><span class="n">regsubsets</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">nvmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">17</span><span class="p">))</span><span class="w">
</span><span class="n">model_lsq_best_selection</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tibble</span><span class="p">(</span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="nf">length</span><span class="p">(</span><span class="n">model_lsq_best_summ</span><span class="o">$</span><span class="n">rss</span><span class="p">)),</span><span class="w">
                                   </span><span class="n">rss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_summ</span><span class="o">$</span><span class="n">rss</span><span class="p">,</span><span class="w">
                                   </span><span class="n">adjr2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_summ</span><span class="o">$</span><span class="n">adjr2</span><span class="p">,</span><span class="w">
                                   </span><span class="n">cp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_summ</span><span class="o">$</span><span class="n">cp</span><span class="p">,</span><span class="w">
                                   </span><span class="n">bic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_summ</span><span class="o">$</span><span class="n">bic</span><span class="p">)</span><span class="w">

    </span><span class="c1"># model performance by RSS, adj R2, cp, and bic</span><span class="w">
  </span><span class="n">p1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">rss</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_line</span><span class="p">()</span><span class="w"> 
  
  </span><span class="n">max_adjr2_loc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.max</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="o">$</span><span class="n">adjr2</span><span class="p">)</span><span class="w"> 
  </span><span class="n">p2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">adjr2</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_line</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">annotate</span><span class="p">(</span><span class="s2">"point"</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_adjr2_loc</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_selection</span><span class="o">$</span><span class="n">adjr2</span><span class="p">[</span><span class="n">max_adjr2_loc</span><span class="p">],</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
  
  </span><span class="n">min_cp_loc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="o">$</span><span class="n">cp</span><span class="p">)</span><span class="w"> 
  </span><span class="n">p3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">cp</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_line</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">annotate</span><span class="p">(</span><span class="s2">"point"</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">min_cp_loc</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_selection</span><span class="o">$</span><span class="n">cp</span><span class="p">[</span><span class="n">min_cp_loc</span><span class="p">],</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
  
  </span><span class="n">min_bic_loc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="o">$</span><span class="n">bic</span><span class="p">)</span><span class="w">
  </span><span class="n">p4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">model_lsq_best_selection</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">bic</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
    </span><span class="n">geom_line</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> 
    </span><span class="n">annotate</span><span class="p">(</span><span class="s2">"point"</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">min_bic_loc</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_lsq_best_selection</span><span class="o">$</span><span class="n">bic</span><span class="p">[</span><span class="n">min_bic_loc</span><span class="p">],</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">

  </span><span class="p">(</span><span class="n">p1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">p3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p4</span><span class="p">)</span><span class="w">
    
</span><span class="n">errors</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="m">17</span><span class="p">)</span><span class="w"> </span><span class="c1"># test data cross validation error</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">17</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">coefi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">model_lsq_best</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="w">
  </span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">test_matrix</span><span class="p">[,</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">coefi</span><span class="p">)]</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">coefi</span><span class="w">
  </span><span class="n">errors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">which.min</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="w"> </span><span class="c1"># 12 - MSE: 1256814</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of predictors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Test MSE"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">
    
</span><span class="n">coef</span><span class="p">(</span><span class="n">model_lsq_best</span><span class="p">,</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span><span class="w">


</span><span class="c1"># least squares - forward selection</span><span class="w">
</span><span class="n">model_lsq_forward</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">regsubsets</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">nvmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">18</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"forward"</span><span class="p">)</span><span class="w">

</span><span class="n">errors</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="m">17</span><span class="p">)</span><span class="w"> </span><span class="c1"># test data cross validation error</span><span class="w">
</span><span class="n">model_rsq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="m">17</span><span class="p">)</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">17</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">coefi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">model_lsq_forward</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="w">
  </span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">test_matrix</span><span class="p">[,</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">coefi</span><span class="p">)]</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">coefi</span><span class="w">
  </span><span class="n">errors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w">
  </span><span class="n">model_rsq</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rsq</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">which.min</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="w"> </span><span class="c1"># 12 - MSE: 1256814</span><span class="w">
</span><span class="n">model_rsq</span><span class="p">[</span><span class="m">12</span><span class="p">]</span><span class="w"> </span><span class="c1"># R-Squared: .9138</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of predictors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Test MSE"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">

</span><span class="n">coef</span><span class="p">(</span><span class="n">model_lsq_forward</span><span class="p">,</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span><span class="w">

  </span><span class="c1">## ALTERNATE WAY - select automatically by AIC</span><span class="w">
</span><span class="n">model_null</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">
</span><span class="n">model_full</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span><span class="w">

</span><span class="n">model_lsq_forward_2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">step</span><span class="p">(</span><span class="n">model_null</span><span class="p">,</span><span class="w"> </span><span class="n">scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_null</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_full</span><span class="p">),</span><span class="w"> </span><span class="n">direction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"forward"</span><span class="p">)</span><span class="w">
</span><span class="n">pred_lsq_forward_2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_lsq_forward_2</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="p">(</span><span class="n">pred_lsq_forward_2</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># MSE: 1259145</span><span class="w">
</span><span class="n">rsq</span><span class="p">(</span><span class="n">pred_lsq_forward_2</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># R-squared: .9136</span><span class="w">

</span><span class="c1"># least squares - backward selection</span><span class="w">
</span><span class="n">model_lsq_backward</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">regsubsets</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">nvmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">18</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"backward"</span><span class="p">)</span><span class="w">

</span><span class="n">errors</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="m">17</span><span class="p">)</span><span class="w"> </span><span class="c1"># test data cross validation error</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">17</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">coefi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">coef</span><span class="p">(</span><span class="n">model_lsq_backward</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="w">
  </span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">test_matrix</span><span class="p">[,</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">coefi</span><span class="p">)]</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">coefi</span><span class="w">
  </span><span class="n">errors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="n">which.min</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span><span class="w"> </span><span class="c1"># 12 - MSE: 1256814</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Number of predictors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Test MSE"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b"</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">

</span><span class="n">coef</span><span class="p">(</span><span class="n">model_lsq_bbackward</span><span class="p">,</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span><span class="w">

</span><span class="c1"># ridge regression</span><span class="w">
</span><span class="n">grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">

</span><span class="n">cv_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">train_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="o">$</span><span class="n">Apps</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">thres</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1e-12</span><span class="p">)</span><span class="w">
</span><span class="n">best_lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv_ridge</span><span class="o">$</span><span class="n">lambda.min</span><span class="w">

</span><span class="n">model_ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">train_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="o">$</span><span class="n">Apps</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_lambda</span><span class="p">,</span><span class="w"> </span><span class="n">thres</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1e-12</span><span class="p">)</span><span class="w">
</span><span class="n">pred_ridge</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_ridge</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_lambda</span><span class="p">,</span><span class="w"> </span><span class="n">newx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_matrix</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="p">(</span><span class="n">pred_ridge</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># MSE: 1261599</span><span class="w">
</span><span class="n">rsq</span><span class="p">(</span><span class="n">pred_ridge</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># R-Squared: .9135</span><span class="w">

</span><span class="c1"># lasso</span><span class="w">
</span><span class="n">cv_lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">train_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="o">$</span><span class="n">Apps</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">thres</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1e-12</span><span class="p">)</span><span class="w">
</span><span class="n">best_lambda_lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv_lasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="w">

</span><span class="n">model_lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">train_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="o">$</span><span class="n">Apps</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_lambda_lasso</span><span class="p">,</span><span class="w"> </span><span class="n">thres</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1e-12</span><span class="p">)</span><span class="w">
</span><span class="n">pred_lasso</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_lasso</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_lambda_lasso</span><span class="p">,</span><span class="w"> </span><span class="n">newx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_matrix</span><span class="p">)</span><span class="w">
</span><span class="n">coef</span><span class="p">(</span><span class="n">model_lasso</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="p">(</span><span class="n">pred_lasso</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># MSE: 1261591</span><span class="w">
</span><span class="n">rsq</span><span class="p">(</span><span class="n">pred_lasso</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># R-Squared: .9135</span><span class="w">

</span><span class="c1"># pcr</span><span class="w">
</span><span class="n">model_pcr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pcr</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"CV"</span><span class="p">)</span><span class="w">
</span><span class="n">validationplot</span><span class="p">(</span><span class="n">model_pcr</span><span class="p">,</span><span class="w"> </span><span class="n">val.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"MSEP"</span><span class="p">)</span><span class="w">

</span><span class="n">pred_pcr</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_pcr</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="p">,</span><span class="w"> </span><span class="n">ncomp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">17</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="p">(</span><span class="n">pred_pcr</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># MSE: 1261630</span><span class="w">
</span><span class="n">rsq</span><span class="p">(</span><span class="n">pred_pcr</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># R-Squared: .9135</span><span class="w">

</span><span class="c1"># pls</span><span class="w">
</span><span class="n">model_pls</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">plsr</span><span class="p">(</span><span class="n">Apps</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"CV"</span><span class="p">)</span><span class="w">
</span><span class="n">validationplot</span><span class="p">(</span><span class="n">model_pls</span><span class="p">,</span><span class="w"> </span><span class="n">val.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"MSEP"</span><span class="p">)</span><span class="w">

</span><span class="n">pred_pls</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model_pls</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="p">,</span><span class="w"> </span><span class="n">ncomp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">9</span><span class="p">)</span><span class="w">

</span><span class="n">mse</span><span class="p">(</span><span class="n">pred_pls</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># MSE: 1286077</span><span class="w">
</span><span class="n">rsq</span><span class="p">(</span><span class="n">pred_pls</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="o">$</span><span class="n">Apps</span><span class="p">)</span><span class="w"> </span><span class="c1"># R-Squared: .9118</span></code></pre></figure>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Source: James et. al, <em>Intro to Statistical Learning 7th edition</em>, pg. 222 <a href="#fnref:1" class="reversefootnote">⤴</a></p>
    </li>
    <li id="fn:2">
      <p>Source: James et. al, <em>Intro to Statistical Learning 7th edition</em>, pg. 226 <a href="#fnref:2" class="reversefootnote">⤴</a></p>
    </li>
    <li id="fn:3">
      <p>Source: James et. al, <em>Intro to Statistical Learning 7th edition</em>, pg. 227 <a href="#fnref:3" class="reversefootnote">⤴</a></p>
    </li>
  </ol>
</div>

          </div>
          <!--
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Linear+Model+Selection+and+Regularization%20-%20http://localhost:4000/posts/linear-model-selection-regularization" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/linear-model-selection-regularization" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>
          -->
          <span class="backbutton"><a href="/posts/index">←Index</a></span>
          
        </article>
        <footer class="footer ">

<small class="pull-left"> &copy;2019 All rights reserved. 

<a href="https://github.com/nielsenramon/chalk" target="_blank">Chalk</a> theme by Nielson Ramon.

</small>
</footer>

      </div>
    </div>
  </main>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160038909-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-160038909-1');
  </script>


<script src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>




<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }
   });
   MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
     var TEX = MathJax.InputJax.TeX;
     var COLS = function (W) {
       var WW = [];
       for (var i = 0, m = W.length; i < m; i++)
         {WW[i] = TEX.Parse.prototype.Em(W[i])}
       return WW.join(" ");
     };
     TEX.Definitions.Add({
       environment: {
         psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
       }
     });
   });
 </script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
