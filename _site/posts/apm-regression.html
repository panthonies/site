<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anthony Pan | Regression Techniques for Predictive Modeling</title>
  <meta name="description" content="Buckle up! This is going to be a long one.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Regression Techniques for Predictive Modeling">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/posts/apm-regression">
  <meta property="og:description" content="Buckle up! This is going to be a long one.">
  <meta property="og:site_name" content="Anthony Pan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://localhost:4000/posts/apm-regression">
  <meta name="twitter:title" content="Regression Techniques for Predictive Modeling">
  <meta name="twitter:description" content="Buckle up! This is going to be a long one.">

  
    <meta property="og:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
    <meta name="twitter:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
  

  <link href="http://localhost:4000/feed.xml" type="application/rss+xml" rel="alternate" title="Anthony Pan Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-3ebe45100a3d97f8ac94857224f3fde7193d453226ebbb900022771bfa032719.css">
    

  

</head>
<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav ">
  <a href="/" class="header-logo" title="Anthony Pan">Anthony Pan</a>
  <ul class="header-links">
    
      <li>
        <a href="/" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="/posts/index" title="Blog">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-writing">
  <use href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing" xlink:href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article ">
          <header class="article-header">
            <h1>Regression Techniques for Predictive Modeling</h1>
            <p>Buckle up! This is going to be a long one.</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    May 18, 2020
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      23 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
      
      <a href="/tag/academic" title="See all posts with tag 'academic'">academic</a>
    
  </div>
</div>
          </header>

          <div class="article-content">
            <div class="toc">
  <h3 id="contents">Contents</h3>
  <ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#i-linear-models">I. Linear Models</a>
    <ul class="hi"><li><a href="#linear-regression">Linear Regression</a></li>
    <li><a href="#partial-least-squares-and-principal-components-regression">Partial Least Squares and Principal Components Regression</a></li>
    <li><a href="#penalized-models-ridge-lasso-and-elastic-net">Penalized Models: Ridge, Lasso, and Elastic Net</a></li></ul></li>

<li><a href="#ii-nonlinear-models">II. Non-Linear Models</a>
    <ul class="hi"><li><a href="#neural-networks">Neural Networks</a></li>
    <li><a href="#multivariate-adaptive-regression-splines-mars">Multivariate Adaptive Regression Splines (MARS)</a></li>
    <li><a href="#support-vector-machines">Support Vector Machines</a></li>
    <li><a href="#k-nearest-neighbors">K-Nearest Neighbors</a></li></ul></li>

<li><a href="#ii-regression-trees-and-rule-based-models">IV. Regression Trees and Rule-Based Models</a>
    <ul class="hi"><li><a href="#basic-regression-trees">Basic Regression Trees</a></li>
    <li><a href="#regression-model-trees">Regression Model Trees</a></li>
    <li><a href="#rule-based-models">Rule-Based Models</a></li>
    <li><a href="#bagged-trees">Bagged Trees</a></li>
    <li><a href="#random-forests">Random Forests</a></li>
    <li><a href="#boosting">Boosting</a></li>
    <li><a href="#cubist">Cubist</a></li></ul></li>
</ul>
</div>

<h2 id="introduction">Introduction</h2>
<hr />
<hr />
<hr />

<p><small>Note: I will try not to repeat details I’ve covered in my previous posts on <a href="linear-regression">linear regression</a>, <a href="linear-model-selection-regularization">regularization</a>, <a href="moving-beyond-linearity">splines and GAMs</a>, <a href="tree-based-methods">tree-based methods</a>, and <a href="support-vector-machines">SVMs</a>. This post focuses more on model application, while my previous posts focus on defining and explaining the method.</small></p>

<p>Regression models predict a numeric outcome. Their performance is usually measured with:</p>
<ul>
  <li><strong>Root Mean Squared Error (RMSE)</strong> is the most common perfromance measure, and represents how far (on average) the residuals are from zero or the average distance between the observed values and model predictions.</li>
  <li><script type="math/tex">R^2</script>, the proportion of outcome variance explained by the model. When interpreting R-squared, remember that the value is dependent on the variation of the outcome – a high value doesn’t necessarily indicate a “good” model unless it meets your specific goals.</li>
  <li>Spearman’s rank correlation is often used if the model is judged by its ability to rank new samples as opposed to numerical predictive accuracy.</li>
</ul>

<p>The <strong>bias-variance tradeoff</strong> can be illustrated with the mean squared error:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
\text{MSE} & = \frac{1}{n} \sum_{i = 1}^n (y_i - \hat y_i) ^ 2 \\
E\left[\text{MSE}\right] & = \sigma^2 + (\text{Model Bias})^2 + \text{Model Variance}
\end{align} %]]></script>

<p>The <strong>bias</strong> reflects how close the functional form of the model can get to the true relationship between the predictors and outcome, and the <strong>variance</strong> reflects how sensitive the model is to additional data points. Generally, a simple model will have low variance and high bias, while a complex model will have high variance and low bias.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="s2">"caret"</span><span class="p">)</span><span class="w">
</span><span class="n">R2</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span><span class="w"> </span><span class="n">observed</span><span class="p">)</span><span class="w">
</span><span class="n">RMSE</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span><span class="w"> </span><span class="n">observed</span><span class="p">)</span><span class="w">

</span><span class="n">cor</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span><span class="w"> </span><span class="n">observed</span><span class="p">)</span><span class="w">
</span><span class="n">cor</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span><span class="w"> </span><span class="n">observed</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"spearman"</span><span class="p">)</span></code></pre></figure>

<hr />
<hr />
<hr />

<h1 id="i-linear-models">I. Linear Models</h1>
<hr />
<hr />
<hr />

<p>Linear regression-type models are highly interpretable, and standard errors can be computed to assess the statistical significance of each predictor. However, they assume that the relationship between the predictors and response falls along a hyperplane. Although linear models can be augmented to capture interactions and higher degree relationships, nonlinear relationships may not be adequately captured.</p>

<h2 id="linear-regression">Linear Regression</h2>
<hr />

<p><em>[Note: I wrote a more comprehensive overview of linear regression</em> <a href="linear-regression"><em>here.</em></a><em>]</em></p>

<p>The objective of ordinary least squares linear regression is to find the plane that minimizes the sum of squared errors between the observed and predicted response.</p>

<p>There are no tuning parameters. However, we must use training and validation techniques to understand its predictive ability. When reampling a dataset with many predictors (for example, 100 samples and 75 predictors), be mindful that resampling may not be able to find a unique set of regression coefficients if the resampling scheme only uses ~2/3 of the data.</p>

<p><span class="boxheader">Drawbacks:</span></p>
<ul>
  <li>Does not work when the number of predictors is greater than the number of samples. If the number of predictors is large, reduce the dimension of the predictor space with techniques such as PCA.</li>
  <li>Not good when a predictor is a linear combination of other predictors. When collinearity exists, then a linear model can still be fit but we lose the ability to meaningfully interpret the coefficients. We can diagnose multicollinearity with the variance inflation factor (VIF).</li>
  <li>Not good for modeling non-linear relationships. Diagnose this with residual plots.</li>
  <li>Not good at handling to outliers, since they have exponentially larger residuals. We can make linear regression more robust by minimizing a metric other than SSE, such as mean absolute error and the Huber function (see below<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>).</li>
</ul>

<center><a href="/assets/05-22-loss-59652711d08f6c7be89abaf48706ef0f3fe31bf1a9b98cbf67f229b57c7aabbe.png">
  <img src="/assets/05-22-loss-59652711d08f6c7be89abaf48706ef0f3fe31bf1a9b98cbf67f229b57c7aabbe.png" alt="loss" class="zooming" data-rjs="/assets/05-22-loss-59652711d08f6c7be89abaf48706ef0f3fe31bf1a9b98cbf67f229b57c7aabbe.png" data-zooming-width="" data-zooming-height="" />
</a>
</center>

<h2 id="partial-least-squares-and-principal-components-regression">Partial Least Squares and Principal Components Regression</h2>
<hr />

<p><em>[Note: I wrote a more comprehensive overview of PLS and PCR</em> <a href="linear-model-selection-regularization#iv-dimension-reduction-techniques"><em>here.</em></a><em>]</em></p>

<p><strong>Partial Least Squares (PLS)</strong> finds linear combinations of the variables chosen to maximally summarize their covariance with the response. PLS is a supervised dimension reduction procedure that should be used when there are correlated predictors and a linear regression-type solution is desired.</p>

<p><strong>Principal Components Regression (PCR)</strong> finds linear combinations of the variables to maximally summarize their variance without any regard to the response. However, if the variability of the predictors is not related to the variability of th eresponse, then PCR can have difficulty identifying a predictive relationship when one might actually exist.</p>

<p>In practice, PLS and PCR have <strong>similar predictive ability</strong>. The number of components needed in PLS is always less than or equal to that of PCA to achieve the same predictive power.</p>

<p><span class="boxheader">Good to know:</span></p>
<ul>
  <li>It’s important to pre-process the data by <strong>centering and scaling</strong> to enure that variation in each variable is treated equally.</li>
  <li><strong>“Variable importance in the projection” (VIP)</strong> can be assessed by the size of the normalized weight and amount of variation in the response that is explained by the component. In other words, the importance of the <script type="math/tex">j</script>th predictor is given by a fraction where:
    <ul>
      <li>the numerator is a weighted sum of the normalized weights corresponding to the <script type="math/tex">j</script>th predictor. The <script type="math/tex">j</script>th normalized weight of the <script type="math/tex">k</script>th component, <script type="math/tex">w_{kj}</script>, is scaled by the amount of variation in the response explained by the <script type="math/tex">k</script>th component.</li>
      <li>the denominator is the total amount of response variation explained by all <script type="math/tex">k</script> components.</li>
      <li>the rule of thumb is that values over 1 have important predictive power.</li>
    </ul>
  </li>
</ul>

<p>There have been many advances in computational time and dealing with nonlinear predictor spaces. PLS and PCR require significant effort to model nonlinear relationships, especially when the number of predictors is large. For cases where non-linear modeling is important, other techniques that can more naturally identify nonlinear structures are recommended over PLS and PCR.</p>

<p>In R, PLS and PCR have been implemented in the <code class="highlighter-rouge">pls</code> package, and can be specified in <code class="highlighter-rouge">caret</code> with method values of <code class="highlighter-rouge">"pls"</code>,<code class="highlighter-rouge">"oscorespls"</code>, <code class="highlighter-rouge">"simpls"</code>, and <code class="highlighter-rouge">"widekernelpls"</code>.</p>

<h2 id="penalized-models-ridge-lasso-and-elastic-net">Penalized Models: Ridge, Lasso, and Elastic Net</h2>
<hr />

<p><em>[Note: I wrote a more comprehensive overview of lasso and ridge regression</em> <a href="linear-model-selection-regularization#iii-shrinkage"><em>here.</em></a><em>]</em></p>

<p>When the linear regression model overfits the data, or when there are issues with collinearity leading to high variance, regularizing the parameter estimates can increase the bias of the model and increase predictive accuracy. It is common that a small increase in bias can produce a substantial drop in variance.</p>

<p><strong>Ridge Regression</strong> imposes an <script type="math/tex">L_2</script> penalty, which signifies that a second order penalty (the square) is being used on the parameter estimates. This shrinks the coefficient estimates as the penalty becomes large, but does not conduct feature selection. Ridge reg</p>

<p><strong>Lasso</strong> imposes an <script type="math/tex">L_1</script> penalty, penalizes the sum of the absolute values of the coefficients, and performs feature selection. The <script type="math/tex">L_1</script> penalty has also been extended for use in LDA, PLS, and PCA.</p>

<p>The <strong>Elastic Net model</strong> is a generalization that combines the <script type="math/tex">L_1</script> and <script type="math/tex">L_2</script> penalties and more effectively deals with groups of high-correlated predictors.</p>

<script type="math/tex; mode=display">\text{SSE}_{\text{Enet}} = \sum_{i = 1}^n (y_i - \hat y_i)^2 + \lambda_1 \sum_{j = 1}^P \beta_j^2 + \lambda_2 \sum_{j = 1}^P \vert \beta_j \vert</script>

<p><span class="boxheader">Good to know:</span></p>
<ul>
  <li>When dealing with <strong>correlated predictors</strong>, ridge regression shrinks their coefficients toward each other, allowing them to borrow strength from each other, while lasso will tend to pick one and ignore the rest.</li>
  <li>Lasso was significantly advanced with the <strong>Least Angle Regression (LARS)</strong> model, which is a broad framework that encompasses the lasso and similar model. LARS can be used to fit lasso models more efficiently, especially in high-dimensional problems.</li>
</ul>

<p>In R, penalized models have been implemented in <code class="highlighter-rouge">MASS</code>, <code class="highlighter-rouge">elasticnet</code>, <code class="highlighter-rouge">glmnet</code>, and many other packages. They can be trained in <code class="highlighter-rouge">caret</code> with method values of <code class="highlighter-rouge">"ridge"</code>, <code class="highlighter-rouge">"lasso"</code>, <code class="highlighter-rouge">"enet"</code>, <code class="highlighter-rouge">"glmnet"</code>, and more. Implementations of variations on lasso include the packages <code class="highlighter-rouge">biglars</code> (large data sets), <code class="highlighter-rouge">FLLat</code> (fused lasso), <code class="highlighter-rouge">grplasso</code> (group lasso), and <code class="highlighter-rouge">relaxo</code> (relaxed lasso).</p>

<hr />
<hr />
<hr />
<h1 id="ii-non-linear-models">II. Non-Linear Models</h1>
<hr />
<hr />
<hr />
<p>In the inherently nonlinear models described below, the exact form of nonlinearity does not need to be known explicitly or specified prior to model training.</p>

<h2 id="neural-networks">Neural Networks</h2>
<hr />

<p>In neural networks, the outcome is modeled by a set of unobserved variables, called hidden units, consisting of linear combinations of the original predictors which have been transformed by a nonlinear function <script type="math/tex">g(\cdot)</script>. There are no constraints on the hidden units, so the coefficients in each unit probably don’t represent any coherent information.</p>

<p>A common neural network implementation has hidden units that have been transformed by the logistic/sigmoidal function:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
h_k(x) & = g \biggl( \beta_{0k} + \sum_{i = 1}^P x_j \beta_{jk} \biggr) \text{, where } \\
g(u) & = \frac{1}{1 + e^{-u}} 
\end{align} %]]></script>

<p>The hidden units then model the outcome with a linear relationship:</p>

<script type="math/tex; mode=display">f(x) = \gamma_0 + \sum_{k = 1}^H \gamma_k h_k</script>

<p>Note that there are a total of <script type="math/tex">H*(P+1) + H + 1</script> parameters. The coefficients and intercepts of the <script type="math/tex">H</script> hidden units have <script type="math/tex">H * (P + 1)</script> parameters, and the final linear relationship has an additional <script type="math/tex">H</script> coefficients plus the intercept.</p>

<p>To minimize the sum of squared residuals, the parameters are usually initialized to random values and optimized. However, It is common that the calculated solution is not a global solution. Neural networks also have a tendency to overfit due to a large number of regression coefficients.</p>

<p><strong>Weight decay</strong> addresses overfitting by adding a penalty for large regression coefficients. They must significantly decrease the error in order to be added to the model. Formally, we add a constraint so that the sum of squared errors of the model for a given <script type="math/tex">\lambda</script> minimizes:</p>

<script type="math/tex; mode=display">\sum_{i = 1}^n (y_i - f_i(x))^2 + \lambda \sum_{k = 1}^ H \sum_{j = 0} ^ P \beta_{jk}^2 + \lambda \sum_{k = 0}^H \gamma_k^2</script>

<p>As <script type="math/tex">\lambda</script> increases, the model becomes more smooth (higher bias); reasonable values of <script type="math/tex">\lambda</script> are between 0 and .1. Be sure to scale and center predictors before using cross validation to select the <strong>two tuning parameters</strong>: the <strong>number of hidden units</strong>, and the <strong>regularization value <script type="math/tex">\lambda</script></strong>.</p>

<p>The model structure described above is a <strong>single-layer feed-forward neural network</strong>, the simplest neural network architecture. There are many extensions to this model, including those that have more than one hidden layer, loops going both directions between layers, and Bayesian approaches. For example, a “self-organizing map” takes a similar approach.</p>

<p><span class="boxheader">Good to know:</span></p>

<ul>
  <li>Use cross-validation to select the two tuning parameters of a single-layer feed-forward neural network: the <strong>number of hidden units</strong>, and the <strong>regularization value <script type="math/tex">\lambda</script></strong> (between 0 and .1).</li>
  <li><strong>Center and scale</strong> the predictors, since the regression coefficients are being summed</li>
  <li>Neural networks are <strong>adversely affected by high correlation</strong> among predictors. To address this, we can pre-filter predictors or use feature extraction (PCA) before applying neural networks.</li>
  <li><strong>Averaging the results</strong> of neural networks initialized with different starting values can significantly improve performance. Do it if possible!</li>
</ul>

<p>In R, neural networks have been implemented in the packages <code class="highlighter-rouge">nnet</code>, <code class="highlighter-rouge">neural</code>, and <code class="highlighter-rouge">RSNNS</code>. In <code class="highlighter-rouge">caret</code>, neural networks can be trained with the methods <code class="highlighter-rouge">"nnet"</code> (from <code class="highlighter-rouge">nnet</code> package) and <code class="highlighter-rouge">"avgNNet"</code> (with model averaging), among others.</p>

<h2 id="multivariate-adaptive-regression-splines-mars">Multivariate Adaptive Regression Splines (MARS)</h2>
<hr />

<p>MARS predicts the outcome by splitting the predictors into piecewise linear models at cut points that achieve the smallest error.</p>

<p>The <strong>hinge function</strong> that splits the predictors can be written as: <script type="math/tex">h(x) \begin{cases} x, x>0 \\ 0, x \leq 0 \end{cases}</script>, and a pair of hinge functions is written as <script type="math/tex">h(x - a); h(a - x)</script> where <script type="math/tex">a</script> is the cut point.</p>

<p><strong>Algorithm for an Additive MARS Model:</strong></p>
<ul>
  <li>consider each data point for each predictor as a cutopint</li>
  <li>choose the cutpoint that achieves the smallest error when fitting piecewise regression</li>
  <li>repeat for additional features until a stopping point</li>
  <li>prune features taht do not contribute significantly to the model equation</li>
</ul>

<p><strong>Tuning parameters:</strong></p>
<ol>
  <li>The degree of features added to the model</li>
  <li>The number of retained terms (choose this with the LOOCV shortcut, defined <a href="resampling-methods">here</a>)</li>
</ol>

<p>In a <strong>second degree MARS model</strong>, conducts the same search of a single term that improves the model. Then, it searches again for a new cut to couple with each of the original features. In effect, this is adding interaction terms to each hinge function. To illustrate: a single cutpoint splits a predictor into hinge functions <script type="math/tex">A</script> and <script type="math/tex">B</script>, then we find two more cutpoints for hinge functions <script type="math/tex">C, D, E,</script> and <script type="math/tex">F</script> such that <script type="math/tex">A \times C, A \times D, B \times E</script>, and <script type="math/tex">B \times F</script> minimize error.</p>

<p><strong>Advantages:</strong></p>
<ul>
  <li>automatic feature selection</li>
  <li>interpretability of predictors</li>
  <li>very little pre-processing required; transformations and filtering predictors are not needed, although correlated predictors can complicate interpretation.</li>
  <li><strong>Variable importance</strong> can be quantified by tracking the reduction in RMSE for each feature.</li>
</ul>

<p>In R, MARS models are implemented in the <code class="highlighter-rouge">earth</code> package. In <code class="highlighter-rouge">caret</code>, they can be trained with the <code class="highlighter-rouge">"earth"</code> method.</p>

<h2 id="support-vector-machines">Support Vector Machines</h2>
<hr />
<p><em>[Note: I wrote a more comprehensive overview of SVM for classification</em> <a href="support-vector-machines"><em>here.</em></a><em>]</em></p>

<p>We use a technique called <script type="math/tex">\epsilon</script>-insensitive regression to define SVMs in the framework of robust regression (minimizing the effect of outliers). The SVM regression coefficients <strong>minimize the cost function</strong>, given an <script type="math/tex">\epsilon</script>-insensitive function <script type="math/tex">L_\epsilon</script> and cost penalty tuning parameter set by the user:</p>

<script type="math/tex; mode=display">\text{Cost Penalty} * \sum_{i = 1}^n L_\epsilon (y_i - \hat y_i) + \sum_{j = 1}^p \beta_j^2</script>

<p>Data points within the threshold (cost) do not contribute to the regression fit, while others contribute a linear scale amount. This sounds unintuitive, but it works – a visualization of the residuals versus its contribution to the regression line is shown below.<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup></p>

<center><a href="/assets/05-22-loss2-34d5d145537d82ef47ffd7aed0cc46f614acc6b486af4a436f9b4447ee8c403c.png">
  <img src="/assets/05-22-loss2-34d5d145537d82ef47ffd7aed0cc46f614acc6b486af4a436f9b4447ee8c403c.png" alt="loss2" class="zooming" data-rjs="/assets/05-22-loss2-34d5d145537d82ef47ffd7aed0cc46f614acc6b486af4a436f9b4447ee8c403c.png" data-zooming-width="" data-zooming-height="" />
</a>
</center>

<p>For a new sample <script type="math/tex">u</script>, the parameter estimates <script type="math/tex">\beta_1 ... \beta_n</script> can be written as functions of a set of unknown parameters <script type="math/tex">\alpha_i</script> and the training data points, so that the SVM prediction equation is:</p>

<script type="math/tex; mode=display">\hat y = \beta_0 + \sum_{i = 1}^n \alpha_i \biggl(\sum_{j = 1}^p x_{ij} u_j \biggr)</script>

<p>Note that <script type="math/tex">\alpha_i = 0</script> where the data is within <script type="math/tex">\pm \epsilon</script> of the regression line. Only the subset of points where <script type="math/tex">\alpha \neq 0</script> are needed for prediction. Those points are called <strong>support vectors</strong>.</p>

<p>The SVM formula can be generalized to:</p>

<script type="math/tex; mode=display">f(u) = \beta_0 + \sum_{i = 1}^n \alpha_i K(x_i, u)</script>

<p>In this formula, <script type="math/tex">K(x_i, u)</script> is the kernel. Popular kernels include linear, polynomial, radial basis, hyperbolic tangent. Radial kernels are very effective, with an additional <script type="math/tex">\sigma</script> parameter that controls scale.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{linear} & =  \sum_{j = 1}^p x_{ij} u_j = \mathbf{x'_i u} \\
\text{polynomial} & = (\phi (\mathbf{x'u}) + 1)^{\text{degree}} \\
\text{radial basis function} & =  \exp {(- \sigma \lVert \mathbf{x - u} \rVert ^ 2)}\\
\text{hyperbolic tangent} & =  \tanh (\sigma (\mathbf{x'u}) + 1) \\
\end{align} %]]></script>

<p><span class="boxheader">Good to know:</span></p>
<ul>
  <li>A large cost means the model is more flexible (high variance, low bias).</li>
  <li>Tuning parameters are: <strong>cost</strong> (for example, between <script type="math/tex">2^{-2} to 2^{10}</script>), and other kernel-specific parameters like scale and degree.</li>
  <li>It is possible to tune over <script type="math/tex">\epsilon</script>, but the general recommendation is to fix <script type="math/tex">\epsilon</script> and tune over other parameters.</li>
  <li>Make sure to <strong>center and scale predictors</strong>, since SVM uses sum of cross products.</li>
</ul>

<p>In R, there are a number of implementations of SVMs including the <code class="highlighter-rouge">e1071</code> and <code class="highlighter-rouge">kernlab</code> (more comprehensive for regression) packages. In <code class="highlighter-rouge">caret</code>, SVMs can be trained with the methods <code class="highlighter-rouge">"svmRadial"</code> (radial basis function), <code class="highlighter-rouge">"svmPoly"</code> (polynomial SVM), and <code class="highlighter-rouge">"svmLinear"</code>.</p>

<h2 id="k-nearest-neighbors">K-Nearest Neighbors</h2>
<hr />

<p>KNN predicts new observations from the K closest samples to that observation. The distance between samples is usually calculated as Euclidian distance:</p>

<script type="math/tex; mode=display">\sqrt {\sum_{j = 1}^p (x_{aj} - x_{bj}) ^ 2}</script>

<p>A generalization of Euclidian is Minkowski distance. Note that when <script type="math/tex">q = 2</script>, the Mindowski distance is the same as Euclidian distance.</p>

<script type="math/tex; mode=display">\biggl( \sum \vert x_{aj} - x_{bj} \vert ^ q \biggr) ^ {\frac{1}{q}}</script>

<p><span class="boxheader">Good to know:</span></p>

<ul>
  <li><strong>Center and scale predictors</strong>, since KNN measures “distance”</li>
  <li>Missing values are problematic (try imputation if possible)</li>
  <li>Choose the tuning parameter <script type="math/tex">K</script> with resampling (smaller values leads to more flexible fits and higher variance)</li>
  <li>Computationally not very efficient, since distances between the new sample and all other samples must be computed</li>
  <li>Remove irrelevant or noisy predictors, since they lead to poor performance with KNN</li>
  <li>Improve predictions by weighting neighbors contribution based on distance</li>
</ul>

<p>In R, KNNs can be trained by <code class="highlighter-rouge">caret</code> with the <code class="highlighter-rouge">"knn"</code> method.</p>

<hr />
<hr />
<hr />
<h1 id="iii-regression-trees-and-rule-based-models">III. Regression Trees and Rule-Based Models</h1>
<hr />
<hr />
<hr />
<p>Tree-based models are popular because they generate a set of conditions that are interpretable and easy to implement, handle many types of data without pre-processing, do not require the user to specify the form of the predictors’ relationship to the response, effectively handle missing data, and implicitly conduct feature selection.</p>

<p>However, trees have the weaknesses of model instability, where slight changes in the data can drastically change the structure of the tree, and single trees have poor performance.</p>

<h2 id="basic-regressions-trees">Basic Regressions Trees</h2>
<hr />

<p>Basic regression trees partition the data into smaller groups that are most homogenous with respect to the response. The <strong>CART (classification and regression tree) methodology</strong> for regression begins with the entire data set, <script type="math/tex">S</script>, and searches every value of every predictor to find the predictor and split value that partitions the data into two groups such that the overall sum of squares error are minimized.</p>

<script type="math/tex; mode=display">SSE = \sum_{i \in S_1} (y_i - \bar y_1)^2 + \sum_{i \in S_2} (y_i - \bar y_2)^2</script>

<p>This method is repeated to grow the tree, and is called <strong>recursive partitioning</strong>.</p>

<p>After the tree is grown, it is pruned to prevent over-fitting with a process called cost-complexity tuning, penalizing the error rate using the size of the tree with a complexity parameter <script type="math/tex">c_p</script>:</p>

<script type="math/tex; mode=display">SSE_{c_p} = SSE + c_p \times (\# \text{Terminal Nodes})</script>

<p>We find the best pruned tree by evaluating the data across a sequence of <script type="math/tex">c_p</script> values with cross-validation, and choosing the tree with either the lowest RMSE or using the one standard-error rule. Predictions are calculated with the average of the training set outcomes in each terminal node.</p>

<p><span class="boxheader">Good to know:</span></p>

<ul>
  <li>CART trees can handle missing data. For each split, alternatives (called surrogate splits) are evaluated whose results are similar to the original splits. It a surrogate split approximates the original split well, then it can be used when the predictor data for the original split is missing.</li>
  <li>Variable importance can be calcaulated by the overall reduction in the optimization criteria for each predictor</li>
  <li>Trees intrinsically conduct feature selection, but the choice of split for highly correlated predictors is somewhat random</li>
  <li><strong>Disadvantages</strong>:
    <ul>
      <li>single trees have sub-optimal predictive performance</li>
      <li>the number of possible predicted outcomes is determined by the number of terminal nodes</li>
      <li>individual trees tend to be unstable</li>
      <li>they suffer from selection bias (predictors with a higher number of distinct values are favored)</li>
      <li>as the number of missing values increases, the selection of preedictors becomes more biased.</li>
    </ul>
  </li>
</ul>

<p>Another approach to the problem is <strong>conditional inference trees</strong>, which uses hypothesis tests to evaluate the difference between the means of each possible split point and computes a p-value that the split leads to a significant improvement. Features of conditional inference trees include:</p>
<ul>
  <li>Predictors on disparate scales are able to be compared, thanks to the p-value.</li>
  <li>Multiple comparison corrections can be applied to p-values to reduce the bias resulting from a large number of split candidates. Predictors are increasingly penalized as the number of splits increases, reducing bias.</li>
  <li>By default, they are not pruned, but their complexity should still be chosen via resampling techniques.</li>
  <li>There is one tuning parameter: the significance threshold to determine whether additional splits should be created.</li>
</ul>

<p>In R, the CART methodology is implemented in the <code class="highlighter-rouge">rpart</code> package, and the conditional inference tree framework is implemented in the <code class="highlighter-rouge">party</code> package. Single regression trees can be trained in <code class="highlighter-rouge">caret</code> using the methods <code class="highlighter-rouge">"rpart"</code> (CART tree tuned over complexity), <code class="highlighter-rouge">"rpart2"</code> (CART tree tuned over maximum depth), <code class="highlighter-rouge">"ctree"</code> (conditional inference tree tuned over the minimum criterion that must be met to continue splitting), and <code class="highlighter-rouge">"ctree2"</code> (conditional inference tree tuned over maximum depth).</p>

<h2 id="regression-model-trees">Regression Model Trees</h2>
<hr />

<p>Simple regression trees use the average of the training set outcomes in each terminal node as the prediction, which often measn that they underpredict samples in the extremes. The <strong>model tree approach, also called M5,</strong> addresses this problem by having its terminal nodes predict the outcome using a linear model.</p>

<p>The initial split is found using an exhaustive search over the predictors and training set samples using the <strong>expected reduction in the node’s error rate</strong>:</p>

<script type="math/tex; mode=display">\text{reduction} = \text{SD}(S) - \sum_{i = 1} ^ P \frac{n_i}{n} \times \text{SD}(S_i)</script>

<p>Where <script type="math/tex">S</script> denotes the entire set of data split into <script type="math/tex">P</script> subsets, <script type="math/tex">\text{SD}</script> is the standard deviation, and <script type="math/tex">n_i</script> is the number of samples in partition <script type="math/tex">i</script>. This determines if the total variation in the splits, weighted by sample size, is lower than in the presplit data.</p>

<p>The <strong>tree is grown recursively based on reducing the overall error rate</strong>, and the error associated with each linear model is used in place of <script type="math/tex">\text{SD}(S)</script> to determine the reduction in error rate for the next split until there are no further improvements or not enough samples to continue.</p>

<p>Next, <strong>each linear model is simplified using an adjusted error rate</strong> that penalizes models with large numbers of parameters. Terms are dropped from the model as long as the adjusted error rate decreases :</p>

<script type="math/tex; mode=display">\text{Adjusted Error Rate} = \frac{n^* + p}{n^* - p} \sum_{i = 1}^{n^*} \vert y_i - \hat y_i \vert</script>

<ul>
  <li><script type="math/tex">n^*</script> is the number of data points used to build the model</li>
  <li><script type="math/tex">p</script> is the number of parameters</li>
</ul>

<p>Model trees also <strong>incorporate smoothing to decrease the potential for over-fitting</strong>. As the sample goes down the branches of the tree, the tree generates a new prediction for each node.The predictions are combined using:</p>

<script type="math/tex; mode=display">\hat y_{(p)} = \frac{n_{(k)} \hat y_{(k)} + c \hat y_{(p)}}{n_{(k)} + c}</script>

<ul>
  <li><script type="math/tex">\hat y_{(k)}</script> is the prediction form the child node</li>
  <li><script type="math/tex">n_{(k)}</script> is the number of training set data points in the child node</li>
  <li><script type="math/tex">\hat y_{(p)}</script> is the prediction from the parent node</li>
  <li><script type="math/tex">c</script> is a constant with a default value of 15.</li>
</ul>

<p>Smoothing can have a significant positive effect on the model when the models across nodes are very different. This is because (1) the number of available training set samples decreases as new splits are added, and (2) the linear models derived by the splitting process may suffer from significant collinearity.</p>

<p>Once the tree is fully grown, it is <strong>pruned back using the adjusted error rate</strong> (similar to CART trees, see previous section).</p>

<p>In R, the main implementation for model trees is accessed with the <code class="highlighter-rouge">RWeka</code> package, and can be tuned in <code class="highlighter-rouge">caret</code> using the method <code class="highlighter-rouge">"M5"</code> to evaluate model trees, rule-based versions, and the use of smoothing/pruning.</p>

<h2 id="rule-based-models">Rule-Based Models</h2>
<hr />

<p>A rule is defined as a distinct path through a tree, and the number of samples it affects is called its coverage. In some cases, rules created by trees may have redundant terms, and it may be advantageous to remove conditions becasue they do not contribute much to the model.</p>

<p>One approach to creating rules from model trees (Holmes et al, 1993) uses the “separate and conquer” strategy, which derives rules from many different model trees:</p>
<ol>
  <li>Create an initial model tree (recommend unsmoothed) and save only the rule with the largest coverage.</li>
  <li>Remove the samples covered by the first rule from the dataset, create another model tree with the remaining data, and save only the rule with the largest coverage.</li>
  <li>Repeat until all the traiing set data have been covered by eat least one rule.</li>
  <li>A new sample is predicted by determining which rule(s) it falls under, then applies the linear model associated with the largest coverage.</li>
</ol>

<p>In R, rule-based models are also part of the <code class="highlighter-rouge">RWeka</code> package, and can be tuned in <code class="highlighter-rouge">caret</code> using the method <code class="highlighter-rouge">"M5Rules"</code>.</p>

<h2 id="bagged-trees">Bagged Trees</h2>
<hr />
<p><em>[Note: I wrote a more comprehensive overview of bagging</em> <a href="tree-based-methods#iv-method-bagging"><em>here.</em></a><em>]</em></p>

<p>Bagging, or bootstrap aggregation, uses bootstrapping with any regression or classification model to construct an ensemble. Bagging trees consists of generating <script type="math/tex">m</script> bootstrap samples of the original data, training an unpruned model on the sample, calculating a prediction for each model for new samples, and averaging all <script type="math/tex">m</script> predictions for the final prediction.</p>

<p>The main choice for bagging is the <strong>number of bootstrap samples to aggregate</strong>. Often, small improvements can be made using bagging ensembles up to size 50, but if performance is not at an acceptable level after ~50 bagging iterations, then try a more powerfully predictive ensemble method such as random forests or boosting.</p>

<p><strong>Advantages of Bagged Models:</strong></p>
<ul>
  <li>Effectively reduces variance of a prediction through the aggregation process. (note: bagging stable, lower variance models like linear regression and MARS offers less improvement in predicive performance).</li>
  <li>They provide their own internal estimate of predictive performance by with predictions for out-of-bag samples.</li>
</ul>

<p><strong>Disadvantages of Bagged Models:</strong></p>
<ul>
  <li>Computational costs and memory requirements increase with the number of samples, but this can be mitigated with parallel computing since bagging is easily parallelized.</li>
  <li>They are much less interpretable, but measures of variable importance can be constructed by combining measures of importance from individual models across the ensemble.</li>
</ul>

<p>In R, the <code class="highlighter-rouge">ipred</code> package can create bagged trees. Several other packages have functions for bagging, including <code class="highlighter-rouge">caret</code>, which can bag many model types with the <code class="highlighter-rouge">bag()</code> function.</p>

<h2 id="random-forests">Random Forests</h2>
<hr />
<p><em>[Note: I also wrote about random forests</em> <a href="tree-based-methods#v-method-random-forests"><em>here.</em></a><em>]</em></p>

<p>The trees in bagging are not independent of each other, since all predictors are considered at every split. This <strong>tree correlation</strong> in bagging prevents it from optimally reducing variance of the predicted values. Random forests reduce correlation among predictors by adding randomness to the tree construction process.</p>

<p><span class="boxheader">Algorithm for building random forests:</span></p>
<ol>
  <li>Select the number of models to build (recommendation: at least 1,000), commonly referred to as <script type="math/tex">m_{try}</script></li>
  <li>Generate a bootstrap sample of the original data, and train an unpruned tree on the bootstrap sample. For each tree split, randomly select <script type="math/tex">% <![CDATA[
k (< P) %]]></script> of the original predictors as options for partitioning the data.</li>
  <li>Repeat the previous step for the predetermined number of times.</li>
</ol>

<p><span class="boxheader">Good to know:</span></p>

<ul>
  <li>Random forests are more computationally efficient than bagging, since the tree-building can be parallel processed and the tree-building only evaluates a fraction of the original predictors</li>
  <li>Like boosting, random forests can be built with CART trees or conditional inference treees as the naive learner.</li>
  <li>The default tuning parameter for regresion, <script type="math/tex">m_{try} = P/3</script>, tends to work well for a quick assessment of random forest performance.</li>
  <li>Variable importance can be calculated by the improvement in node purity based on the performance metric for each predictor across the forest.
    <ul>
      <li>However, correlations between variables have significant impact on the importance values. Correlations dilute the importance of key predictors, and uninformed predictors can have large importance values if correlated to important variables.</li>
      <li>In addition, the <script type="math/tex">m_{try}</script> tuning parameter can also have a serious effect on importance values.</li>
    </ul>
  </li>
</ul>

<p>In R, the main implementation for random forest comes from the <code class="highlighter-rouge">randomForest</code> package. <code class="highlighter-rouge">Caret</code> can train random forests over <script type="math/tex">m_{try}</script> and the number of bootstrap samples with the <code class="highlighter-rouge">"rf"</code> (CART trees) or <code class="highlighter-rouge">"cforest"</code> (conditional inference trees) methods.</p>

<h2 id="boosting">Boosting</h2>
<hr />
<p><em>[Note: I wrote a more comprehensive overview of boosting</em> <a href="tree-based-methods#vi-method-boosting"><em>here.</em></a><em>]</em></p>

<p>Given a loss function (squared error) and a weak learner (regression trees), find an additive model that minimizes the loss function. The algorithm is initialized with the best guess of the response (mean of repsonse in regression), the gradient (residual) is calculated, and a model is fit to the residuals to minimize the loss function. The current model is added to the previous model, and the procedure continues for a specified number of iterations.</p>

<p>Trees are an excellent base learner because their depth is flexible, they’re easily added togehter, and can be generated very quickly. The two tuning parameters of simple gradient boosting are <strong>tree depth (interaction depth)</strong> and the <strong>number of iterations</strong>.</p>

<p>Unlike random forests, which creates independent trees with maximum depth that contribute equally to the final model, the boosting creates trees dependent on past trees with minimum depth that contribute unequally to the final model. The computational time for boosting is often greater than for random forests, since random forests can be easily parallel processed.</p>

<p>The <strong>learning rate</strong>, <script type="math/tex">\lambda</script>, addresses the greediness of boosting by employing regularization (shrinkage) and adding only a fraction of the current predicted value to the previous iteration’s predicted value.</p>
<ul>
  <li><script type="math/tex">\lambda</script> takes a value between 0 and 1.</li>
  <li>Small values of <script type="math/tex">\lambda</script> (<script type="math/tex">% <![CDATA[
<0.01 %]]></script>) work best, but the value of the parameter is inversely proportional to the computation time required because more iterations are necessary, and more memory is required for storing the model.</li>
</ul>

<p>In stochastic gradient boosting, a <strong>fraction of the data is randomly selected</strong> for each tree-building iteration. This improves accuracy while reducing computational resources. A value of around <script type="math/tex">0.5</script> is recommended, but the fraction can be tuned like any other parameter.</p>

<p><strong>Variable importance</strong> is a function of the reduction in squared error. The importance in squared error due to each predictor is summed within each tree in the ensemble, and averaged across the entire ensemble to yield an overall importance value. Compared to random forests, the importance profile for boosting will often have a much steeper importance slope because the trees from boosting are dependent on each other and have correlated structures. Many of the same predictors will be selected across trees.</p>

<p>In R, the <code class="highlighter-rouge">gbm</code> package implements stochastic gradient boosting. In <code class="highlighter-rouge">caret</code>, stochastic graient boosting can be trained with the <code class="highlighter-rouge">"gbm"</code> method, and tuned over the number of trees, interaction depth, shrinkage, and proportion of observations to be sampled.</p>

<h2 id="cubist">Cubist</h2>
<hr />

<p>Cubist is a complicated rule-based model that has several changes compared to model trees and rule-based trees:</p>

<ul>
  <li>different techniques for linear model smoothing, creating rules, and pruning</li>
  <li>optional boosting-like procedure called <em>committees</em></li>
  <li>generated predictions can be adjusted using nearby points from the training set data</li>
</ul>

<p>I’m not going to completely summarize this model, so read Chapter 8.7 of <em>Applied Predictive Modeling</em> and check out <a href="www.RuleQuest.com">www.RuleQuest.com</a> for more details.</p>

<p><span class="boxheader">The Algorithm:</span></p>

<ol>
  <li><strong>Build an <a href="apm-regression#regression-model-trees">M5 model tree</a></strong> with the usual method, splitting predictors by reducing the overall error rate, simplifying with the adjusted error rate, and incorporate smoothing.
    <ul>
      <li>The smoothing process is a linear combination of two models</li>
      <li>The smoothing coefficient is determined by the variance and covariance between the two models’ residuals, giving the model with the smallest RMSE a higher weight.</li>
    </ul>
  </li>
  <li>The final model is used to construct the initial set of rules. The <strong>sequence of linear models at each node is collected into a single, smoothed representation</strong> of the models.
    <ul>
      <li>The adjusted error rate is used to prune or combine rules.</li>
      <li>If the adjusted error rate does not increase from the deletion of a condition or rule, that condition or rule is dropped.</li>
    </ul>
  </li>
  <li><strong>Model committees are created by generating a sequence of rule-based models.</strong> The training set outcome is adjusted based on the prior model fit, and then builds a new set of rules using the pseudo-response.
    <ul>
      <li>Underpredicted points will have increased sample values in hopes that the model will produce a larger prediction in the next iteration.</li>
      <li>Overpredicted points will have decreased sample values in hopes that the model will produce a smaller prediction in the next iteration.</li>
    </ul>
  </li>
  <li>Once the full set of model committees are created, <strong>new samples are predicted using each model and averaged</strong> for the final rule-based prediction.</li>
  <li>After the rule-based model is finalized, Cubist can <strong>adjust the prediction with the <script type="math/tex">K</script> most similar neighbors</strong> from the training set weighted by distance.
    <ul>
      <li>Cubist uses Manhattan (city-block) distances to determine nearest neighbors.</li>
      <li>Neighbors are only included if they are “close enough” to the prediction sample.</li>
      <li>Custom weights are also computed based on the distance to each neighbor.</li>
    </ul>
  </li>
</ol>

<p><strong>Tuning parameters</strong>:</p>
<ul>
  <li>The # of committees (try checking 0 to 100)</li>
  <li>The # of neighbors (checking 0 to 9)</li>
</ul>

<p>There is no established method for measuring variable importance in Cubist models.</p>

<p>In R, the <code class="highlighter-rouge">Cubist</code> package has an implementation of the model. In <code class="highlighter-rouge">caret</code>, Cubist can be tuned over the number of committees and neighbors with the <code class="highlighter-rouge">"cubist"</code> method.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Source: Kuhn and Johnson, <em>Applied Predictive Modeling</em> (2013), pg. 110 <a href="#fnref:1" class="reversefootnote">⤴</a></p>
    </li>
    <li id="fn:2">
      <p>Source: Kuhn and Johnson, <em>Applied Predictive Modeling</em> (2013), pg. 154 <a href="#fnref:2" class="reversefootnote">⤴</a></p>
    </li>
  </ol>
</div>

          </div>
          <!--
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Regression+Techniques+for+Predictive+Modeling%20-%20http://localhost:4000/posts/apm-regression" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/apm-regression" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>
          -->
          <span class="backbutton"><a href="/posts/index">←Index</a></span>
          
        </article>
        <footer class="footer ">

<small class="pull-left"> &copy;2019 All rights reserved. 

<a href="https://github.com/nielsenramon/chalk" target="_blank">Chalk</a> theme by Nielson Ramon.

</small>
</footer>

      </div>
    </div>
  </main>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160038909-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-160038909-1');
  </script>


<script src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>




<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }
   });
   MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
     var TEX = MathJax.InputJax.TeX;
     var COLS = function (W) {
       var WW = [];
       for (var i = 0, m = W.length; i < m; i++)
         {WW[i] = TEX.Parse.prototype.Em(W[i])}
       return WW.join(" ");
     };
     TEX.Definitions.Add({
       environment: {
         psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
       }
     });
   });
 </script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
