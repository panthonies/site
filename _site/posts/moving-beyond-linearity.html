<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anthony Pan | Moving Beyond Linearity</title>
  <meta name="description" content="Linear models can have significant limitations in terms of predictive power, because the linear assumption can be a poor assumption. Methods such as polynomial regression, step functions, regression and smoothing splines, local regression, and generalized additive models can help us flexibly model non-linear relationships.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Moving Beyond Linearity">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://localhost:4000/posts/moving-beyond-linearity">
  <meta property="og:description" content="Linear models can have significant limitations in terms of predictive power, because the linear assumption can be a poor assumption. Methods such as polynomial regression, step functions, regression and smoothing splines, local regression, and generalized additive models can help us flexibly model non-linear relationships.">
  <meta property="og:site_name" content="Anthony Pan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="http://localhost:4000/posts/moving-beyond-linearity">
  <meta name="twitter:title" content="Moving Beyond Linearity">
  <meta name="twitter:description" content="Linear models can have significant limitations in terms of predictive power, because the linear assumption can be a poor assumption. Methods such as polynomial regression, step functions, regression and smoothing splines, local regression, and generalized additive models can help us flexibly model non-linear relationships.">

  
    <meta property="og:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
    <meta name="twitter:image" content="http://localhost:4000/assets/anthony-pan-8360313f1fe7e4685c1a12403f392eb92d3b49dbd4ccfa196dbebf0ddb4ba974.jpg">
  

  <link href="http://localhost:4000/feed.xml" type="application/rss+xml" rel="alternate" title="Anthony Pan Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-3ebe45100a3d97f8ac94857224f3fde7193d453226ebbb900022771bfa032719.css">
    

  

</head>
<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav ">
  <a href="/" class="header-logo" title="Anthony Pan">Anthony Pan</a>
  <ul class="header-links">
    
      <li>
        <a href="/" title="About me">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
  <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="/posts/index" title="Blog">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-writing">
  <use href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing" xlink:href="/assets/writing-b6140719fc6ec0b803300a902e95d85fe15bbc322fb150e13e61ff0be4439d3b.svg#icon-writing"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article ">
          <header class="article-header">
            <h1>Moving Beyond Linearity</h1>
            <p>Linear models can have significant limitations in terms of predictive power, because the linear assumption can be a poor assumption. Methods such as polynomial regression, step functions, regression and smoothing splines, local regression, and generalized additive models can help us flexibly model non-linear relationships.</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    April 30, 2020
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      14 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
      
      <a href="/tag/academic" title="See all posts with tag 'academic'">academic</a>
    
  </div>
</div>
          </header>

          <div class="article-content">
            <div class="toc">
  <h3 id="contents">Contents</h3>

  <ul>
    <li><a href="#i-basis-functions">I. Basis Functions</a></li>
    <li><a href="#ii-regression-splines">II. Regression Splines</a></li>
    <li><a href="#iii-smoothing-splines">III. Smoothing Splines</a></li>
    <li><a href="#iv-local-regression">IV. Local Regression</a></li>
    <li><a href="#v-generalized-additive-models">V. Generalized Additive Models</a></li>
    <li><a href="#vi-applications-in-r">VI. Applications in R</a></li>
  </ul>

</div>

<h3 id="i-basis-functions">I. Basis Functions</h3>

<p>Basis functions are a family of functions of transformations, <script type="math/tex">b_1 (X), ... , b_k (X)</script>, applied to <script type="math/tex">X</script> to and take the form:</p>

<script type="math/tex; mode=display">y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_1(x_i) + ... + \beta_k b_k(x_i) + \epsilon_i</script>

<p>Two very common types of basis functions are <strong>polynomial regression</strong>, with <script type="math/tex">b_j(x_i) = x_i^j</script>, and <strong>piecewise constant step functions</strong>, with <script type="math/tex">% <![CDATA[
b_j(x_i) = I(c_j \leq x_i < c_{j+1}) %]]></script>.</p>

<p><span class="boxheader">Polynomial Regression</span></p>

<script type="math/tex; mode=display">y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + ... + \beta_d x_i^d + \epsilon_i</script>

<p>This is a simple way to extend the linearity assumption. However, it’s unusual to use powers of greater than 3 or 4 due to weirdness at the boundaries.</p>

<p>Note that the variance, <script type="math/tex">Var(\hat f (x_0))</script> can be computed with the variance estimates for each of the coefficients <script type="math/tex">\hat \beta_j</script> and the covariances between pairs of the coefficient estimates.<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<p><span class="boxheader">Step Functions</span></p>

<p>Step functions break <script type="math/tex">X</script> into bins, so it becomes an ordered categorical variable. To create a step function, create cutpoints <script type="math/tex">c_1, c_2, ..., c_k</script>, then create new variables:</p>
<ul>
  <li><script type="math/tex">% <![CDATA[
C_0(x) = I(X < c_1) %]]></script> ;</li>
  <li><script type="math/tex">% <![CDATA[
C_1(x) = I(c_1 \leq X < c_2) %]]></script> ;</li>
  <li>…</li>
  <li><script type="math/tex">C_1(x) = I(c_k \leq X)</script> ;</li>
</ul>

<p><script type="math/tex">I()</script> is an indicator function that returns a 1 if the condition is true, and 0 otherwise. We use least squares to fit a linear model using each of these new variables as predictors:</p>

<script type="math/tex; mode=display">y_i = \beta_0 + \beta_1 C_1 (x_i) + \beta_2 C_2 (x_i) + ... + \beta_K C_K (x_i) + \epsilon_i</script>

<p>These functions are popular in biostatistics and epidemiology, but one downside is that it can miss important action in the data unless there are natural breakpoints.</p>

<hr />
<hr />
<hr />
<h3 id="ii-regression-splines">II. Regression Splines</h3>

<p>Regression splines are also a class of basis functions that extend upon polynomial regression and piecewise constant regression methods in the previous section, but I’m giving them their own section because they are more complicated and deserve some more space!</p>

<p>Splines of degree <script type="math/tex">d</script> are <strong>piecewise degree-<script type="math/tex">d</script> polynomials</strong> with <strong>continuity in derivatives up to degree <script type="math/tex">d - 1</script> at each knot</strong> to ensure continuity (knots are the breakpoints where the polynomial coefficients change).</p>

<p>A cubic spline with <script type="math/tex">k</script> knots uses a total of <script type="math/tex">4+k</script> degrees of freedom, with continuity of the first two derivatives. This is because piecewise cubic functions have a total of <script type="math/tex">k+1</script> sections, each section adds 4 degrees of freedom (one for each coefficient), and each knot has 3 contraints (continuity of the function, its 1st derivative, and its 2nd derivative).</p>

<p><strong>Example: A Cubic Spline Representation</strong></p>

<script type="math/tex; mode=display">y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 h(x_i, \xi_1) + \beta_5 h(x_i, \xi_2)  + \epsilon_i</script>

<p>This is a representation of a cubic spline with two knots, <script type="math/tex">\xi_1</script> and <script type="math/tex">\xi_2</script>. We add one truncated power basis per knot to guarantee our continuity constraints,<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> for a total of <script type="math/tex">k+3</script> basis functions. In this case, 2 knots + 3 degrees of the polynomial = 5 total basis functions.</p>

<p>The truncated power basis is defined as: <script type="math/tex">% <![CDATA[
h(x, \xi) = (x - \xi)^3_{+} = 
\begin{cases} (x - \xi)^3 & \text{if } x > \xi \\
0 & \text{otherwise} 
\end{cases} %]]></script></p>

<p>In other words, fitting a cubic spline means that we’re performing least squares regression with the predictors: <script type="math/tex">\text{ intercept, } X, X^2, X^3, h(x, \xi_1), ..., h(x, \xi_k)</script>. Note that there are a total of <script type="math/tex">k+4</script> coefficients.</p>

<p>A <strong>natural spline</strong> is a regression spline that is required to be linear at the boundary, which provides more stable estimates at the boundaries. The additional boundary constraints lead to relatively narrower confidence intervals at the boundaries.</p>

<p><span class="boxheader">Important Information about Regression Splines:</span></p>
<ul>
  <li>In practice, it’s common to place knots uniformly along the data</li>
  <li>Use cross-validation to choose the number of knots, <script type="math/tex">k</script> based on the value that minimizes RSS.</li>
  <li>In general, regression splines give superior results to polynomial regression, and also gives more stable estimates at boundaries.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="iii-smoothing-splines">III. Smoothing Splines</h3>

<p>In minimizing RSS, we want to find some function <script type="math/tex">g(x)</script> that fits the observed data well, but we don’t want it to interpolate all of the data, since it overfits the data by being too flexible. Instead, we want to find <script type="math/tex">g(x)</script> that makes RSS small, but is also sufficiently smooth.</p>

<p>We ensure that <script type="math/tex">g(x)</script> is smooth by minimizing the loss function:</p>

<script type="math/tex; mode=display">\sum_{i = 1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt</script>

<p>Where <script type="math/tex">\lambda</script> is a non-negative tuning parameter that controls the bias-variance trade-off. If <script type="math/tex">\lambda = 0</script>, then the smoothing spline will be a 100% fit to the data, and as <script type="math/tex">\lambda \to \infty</script>, the model will be a straight line.</p>

<p>This takes on the “Loss + Penalty” form that we also see in ridge regression and lasso. The second half of the equation above is a penalty term that penalizes variability in the model, since the second derivative <script type="math/tex">g''(t)</script> measures the “roughness” of the model, and is large if <script type="math/tex">g</script> is wiggly.</p>

<p><span class="boxheader">Special Properties of the Smoothing Spline</span></p>

<p>The smoothing spline is a <strong>natural cubic spline</strong>. Compared to the basis function approach of a regression spline, the smoothing spline is a <strong>shrunken version</strong>, where <script type="math/tex">\lambda</script> controls the levels of shrinkage.</p>

<ul>
  <li>it is a piecewise cubic polynomial with continuous 1st and 2nd derivatives</li>
  <li>knots at the unique values of <script type="math/tex">x_1, ..., x_n</script></li>
  <li>linear in regions outside of the extreme knots</li>
</ul>

<p><strong>Choosing the tuning parameter <script type="math/tex">\lambda</script>:</strong> As <script type="math/tex">\lambda</script> increases from <script type="math/tex">0</script> to <script type="math/tex">\infty</script>, the effective degrees of freedom <script type="math/tex">\text{df}_\lambda</script> decreases from <script type="math/tex">n</script> to <script type="math/tex">2</script>. We use effective degrees of freedom since although a smoothing spline has <script type="math/tex">n</script> parameters and hence <script type="math/tex">n</script> nominal degrees of freedom, those parameters are heavily constrained or shrunk down. <script type="math/tex">\text{df}_\lambda</script> is a better measure of the flexbility of a smoothing spline.</p>

<p>We define the effective degrees of freedom as <script type="math/tex">\text{df}_\lambda = \sum_{i = 1}^n \{ S_\lambda \}_{ii}</script> ; or the sum of the diagonal elements of <script type="math/tex">S_\lambda</script> where <script type="math/tex">S_\lambda^{n x n} * y = \hat g_\lambda</script>, the solution to a particular <script type="math/tex">\lambda</script>.</p>

<p>Leave-one-out cross validation is very efficient (the same cost as computing a single fit) for choosing <script type="math/tex">\lambda</script>:</p>

<script type="math/tex; mode=display">\text{RSS}_{CV(\lambda)} = \sum_{i = i}^n \bigl( (y_i - \hat g_\lambda^{(-i)} (x_i) \bigr) ^ 2 = \sum_{i = 1}^n \Biggl[ \frac {y_i - \hat g_{\lambda} (x_i)}  {1 - \{ S_\lambda \}_{ii}} \Biggr] ^ 2</script>

<p>Where <script type="math/tex">\hat g_\lambda^{(-i)} (x_i)</script> indicates the fitted value for this smoothing spline evaluated at <script type="math/tex">x_i</script>, where the fit uses all of the training observations except for the <script type="math/tex">i</script>th observation <script type="math/tex">(x_i, y_i)</script>. Note that we can compute all of the LOOCV fits using only the original fit to all of the data!</p>

<hr />
<hr />
<hr />
<h3 id="iv-local-regression">IV. Local Regression</h3>

<p>For local regression, we compute the fit at a target point <script type="math/tex">x_0</script> using only the nearby training observations.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Gather the fraction <script type="math/tex">s = \frac{k}{n}</script> of training points whose <script type="math/tex">x_i</script> are closest to <script type="math/tex">x_0</script></li>
  <li>Assign a weight <script type="math/tex">K_{i0} = K(x_i, x_0)</script> to each point in the neighborhood where the closest has the highest weight, the farthest has 0 weight, and all other points have 0 weight.</li>
  <li>Fit a weighted least squares of <script type="math/tex">y</script> onto <script type="math/tex">x_i</script>: <script type="math/tex">\min_{\hat \beta_0, \hat \beta_1} \sum_{i = 1}^n K_{i0} (y_0 - \beta_0 - \beta_1 x_i) ^ 2</script></li>
  <li>The fitted value at <script type="math/tex">x_0</script> is <script type="math/tex">\hat f (x_0) = \hat \beta_0 + \hat \beta_1 x_0</script></li>
</ol>

<p><strong>Notes:</strong></p>
<ul>
  <li>This is a memory-based procedure, which means it needs all training data for each calculation of a prediction.</li>
  <li>The span, <script type="math/tex">s</script>, is the tuning parameter that we choose based on cross-validation. A small span means the fit is more local and wiggly with higher bias/lower variance, while a large span will lead to a more global fit with lower bias/higher variance.</li>
  <li>Local regression can be used for 1, 2, or 3 variables, but is bad when there are more than 3 or 4 variables due to the curse of dimensionality.</li>
  <li>This can also be generalized to a multiple linear regression with some global and some local variables. These are called <em>varying coefficient models</em>.</li>
</ul>

<hr />
<hr />
<hr />
<h3 id="v-generalized-additive-models">V. Generalized Additive Models</h3>

<p>Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables while maintaining additivity.</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Allows fitting a non-linear <script type="math/tex">f_j</script> to each <script type="math/tex">X_j</script> to model non-linear relationships. This means we don’t have to try transformations on every variable</li>
  <li>Can make more accurate predictions of <script type="math/tex">Y</script></li>
  <li>Can examine the effect of each <script type="math/tex">X_j</script> on <script type="math/tex">Y</script> individually, which is good for inference and interpretability</li>
  <li>Smoothness of <script type="math/tex">f_j</script> for variable <script type="math/tex">X_j</script> can be summarized with the degrees of freedom.</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Models are restricted to being additive, so important interactions can be missed. However, we can add interactions terms or functions manually to fix this problem.</li>
</ul>

<p>For <strong>regression problems</strong>, models can combine natural splines, smoothing splines, local regression, polynomial regression, etc. This is a compromise between linear models and fully nonparametric models, and can be represented by:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
y & = \beta_0 + \sum_{j = 1}^p f_j (x_{ij}) + \epsilon_i \\
& = \beta_0 + f_1 (x_{i1}) + f_2 (x_{i2}) + ... + f_p (x_{ip}) + \epsilon_i 
\end{align} %]]></script>

<p>For <strong>classification problems</strong>, logistic GAMs can be represented by:</p>

<script type="math/tex; mode=display">\log \Biggl( \frac{p(x)} {1 - p(x)} \Biggr) = \beta_0 + f_1(x) + f_2(x_2) + ... + f_p (x_p)</script>

<p>We can use ANOVA to test if certain coefficients in generalized additive models are significant.</p>

<hr />
<hr />
<hr />
<h3 id="vi-applications-in-r">VI. Applications in R</h3>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="s2">"ISLR"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"splines"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"gam"</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># polynomial regression - compare degrees</span><span class="w">
</span><span class="n">fit_poly1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">education</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">fit_poly2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">education</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">poly</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="m">2</span><span class="p">),</span><span class="n">data</span><span class="o">=</span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">fit_poly3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">education</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">poly</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="m">3</span><span class="p">),</span><span class="n">data</span><span class="o">=</span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">fit_poly4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">education</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">poly</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="m">4</span><span class="p">),</span><span class="n">data</span><span class="o">=</span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">fit_poly5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">education</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">poly</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="m">5</span><span class="p">),</span><span class="n">data</span><span class="o">=</span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">anova</span><span class="p">(</span><span class="n">fit_poly1</span><span class="p">,</span><span class="n">fit_poly2</span><span class="p">,</span><span class="n">fit_poly5</span><span class="p">,</span><span class="n">fit_poly5</span><span class="p">,</span><span class="n">fit_poly5</span><span class="p">)</span><span class="w"> </span><span class="c1"># quadratic fit is sufficient</span><span class="w">

</span><span class="c1"># step function using cut()</span><span class="w">
</span><span class="n">fit_step</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">cut</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit_step</span><span class="p">)</span><span class="w">

</span><span class="c1"># regression splines with bs() - generate basis function with specified knots</span><span class="w">
</span><span class="n">fit_regspline</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">bs</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">knots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">25</span><span class="p">,</span><span class="w"> </span><span class="m">40</span><span class="p">,</span><span class="w"> </span><span class="m">60</span><span class="p">)),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit with pre-specified knots</span><span class="w">
</span><span class="n">fit_regspline</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">bs</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit with pre-specified df, knots at uniform quantiles</span><span class="w">

</span><span class="n">agelims</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">range</span><span class="p">(</span><span class="n">Wage</span><span class="o">$</span><span class="n">age</span><span class="p">)</span><span class="w">
</span><span class="n">age_grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="n">from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">agelims</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="n">to</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">agelims</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="w">
</span><span class="n">pred_regspline</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">fit_regspline</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">age_grid</span><span class="p">),</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">Wage</span><span class="o">$</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">Wage</span><span class="o">$</span><span class="n">wage</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gray"</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span><span class="w"> </span><span class="n">pred_spline</span><span class="o">$</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span><span class="w"> </span><span class="n">pred_spline</span><span class="o">$</span><span class="n">fit</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pred_spline</span><span class="o">$</span><span class="n">se</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span><span class="w"> </span><span class="n">pred_spline</span><span class="o">$</span><span class="n">fit</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pred_spline</span><span class="o">$</span><span class="n">se</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w">

</span><span class="c1"># natural splines with ns()</span><span class="w">
</span><span class="n">fit_natspline</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">ns</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit with pre-specified df</span><span class="w">

</span><span class="n">pred_natspline</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">fit_natspline</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">age</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">age_grid</span><span class="p">),</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">age_grid</span><span class="p">,</span><span class="w"> </span><span class="n">pred_natspline</span><span class="o">$</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w">

</span><span class="c1"># smoothing splines with smooth.spline()</span><span class="w">
</span><span class="n">fit_smoothspline</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">smooth.spline</span><span class="p">(</span><span class="n">Wage</span><span class="o">$</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">Wage</span><span class="o">$</span><span class="n">wage</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">16</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit with pre-specified df </span><span class="w">

</span><span class="n">fit_smoothspline_cv</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">smooth.spline</span><span class="p">(</span><span class="n">Wage</span><span class="o">$</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">Wage</span><span class="o">$</span><span class="n">wage</span><span class="p">,</span><span class="w"> </span><span class="n">cv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="c1"># fit with LOOCV-chosen df</span><span class="w">
</span><span class="n">fit_smoothspline_cv</span><span class="o">$</span><span class="n">df</span><span class="w">

</span><span class="c1"># local regression with loess()</span><span class="w">
</span><span class="n">fit_loess</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">loess</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">span</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">fit_loess</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">loess</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">span</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">

</span><span class="c1">### GAMs</span><span class="w">

</span><span class="c1"># use lm() to model GAMs that can be written out with basis functions</span><span class="w">
</span><span class="n">gam_basic</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">ns</span><span class="p">(</span><span class="n">year</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ns</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w"> 
</span><span class="n">plot.Gam</span><span class="p">(</span><span class="n">gam_basic</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w">

</span><span class="c1"># use gam() when using components without basis function form. i.e. s() fits smoothing splines</span><span class="w">
</span><span class="n">gam_3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gam</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">year</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">gam_3</span><span class="p">)</span><span class="w">

</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">gam_3</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">

</span><span class="c1"># compare GAMs against each other and to a linear fit</span><span class="w">
</span><span class="n">gam_1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gam</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">gam_2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gam</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">year</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">anova</span><span class="p">(</span><span class="n">gam_1</span><span class="p">,</span><span class="w"> </span><span class="n">gam_2</span><span class="p">,</span><span class="w"> </span><span class="n">gam_3</span><span class="p">,</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="o">=</span><span class="s2">"F"</span><span class="p">)</span><span class="w"> </span><span class="c1"># gam_2 is preferred</span><span class="w">

</span><span class="c1"># gam with local regression with lo()</span><span class="w">
</span><span class="n">gam_lo1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gam</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">year</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">lo</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">span</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.7</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">gam_lo2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gam</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">lo</span><span class="p">(</span><span class="n">year</span><span class="p">,</span><span class="w"> </span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">span</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w"> </span><span class="c1"># local regression with interaction terms</span><span class="w">

</span><span class="n">library</span><span class="p">(</span><span class="s2">"akima"</span><span class="p">)</span><span class="w"> </span><span class="c1"># to plot local regression interaction</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">gam_lo2</span><span class="p">)</span><span class="w">

</span><span class="c1"># logsitic regression GAM</span><span class="w">
</span><span class="n">gam_logit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gam</span><span class="p">(</span><span class="n">I</span><span class="p">(</span><span class="n">wage</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">250</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">year</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">education</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binomial</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Wage</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">gam_logit</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"green"</span><span class="p">)</span></code></pre></figure>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For the degree 4 case, <script type="math/tex">Var[\hat f(x_0)] = l_0^T \hat C l_0</script>, where <script type="math/tex">\hat C</script> is the 5x5 covariance matrix of the <script type="math/tex">\hat \beta_j</script> and <script type="math/tex">l_0^T = (1, x_0, x_0^2, x_0^3, x_0^4)</script> <a href="#fnref:1" class="reversefootnote">⤴</a></p>
    </li>
    <li id="fn:2">
      <p>It is possible to prove that adding a term of the form <script type="math/tex">\beta_4 h(x, \xi)</script> to the model for a cubic polynomial will lead to a discontinuity in only the third derivative at <script type="math/tex">\xi</script>, and the function will remain continuous, with conitnuous first and second derivatives, at each of the knots. <a href="#fnref:2" class="reversefootnote">⤴</a></p>
    </li>
  </ol>
</div>

          </div>
          <!--
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Moving+Beyond+Linearity%20-%20http://localhost:4000/posts/moving-beyond-linearity" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/moving-beyond-linearity" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>
          -->
          <span class="backbutton"><a href="/posts/index">←Index</a></span>
          
        </article>
        <footer class="footer ">

<small class="pull-left"> &copy;2019 All rights reserved. 

<a href="https://github.com/nielsenramon/chalk" target="_blank">Chalk</a> theme by Nielson Ramon.

</small>
</footer>

      </div>
    </div>
  </main>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160038909-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-160038909-1');
  </script>


<script src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>




<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }
   });
   MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
     var TEX = MathJax.InputJax.TeX;
     var COLS = function (W) {
       var WW = [];
       for (var i = 0, m = W.length; i < m; i++)
         {WW[i] = TEX.Parse.prototype.Em(W[i])}
       return WW.join(" ");
     };
     TEX.Definitions.Add({
       environment: {
         psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
       }
     });
   });
 </script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
